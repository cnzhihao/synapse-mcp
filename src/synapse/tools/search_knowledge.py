"""
AIè¯­ä¹‰æœç´¢å·¥å…· for Synapse MCP

æœ¬æ¨¡å—å®ç°äº†åŸºäºAIè¯­ä¹‰ç†è§£çš„çŸ¥è¯†åº“æœç´¢åŠŸèƒ½ï¼Œå®Œå…¨æ›¿æ¢äº†ä¼ ç»Ÿçš„å…³é”®è¯åŒ¹é…æœç´¢ã€‚

æ ¸å¿ƒåŠŸèƒ½ï¼š
1. AIè¯­ä¹‰æœç´¢ - åˆ©ç”¨è°ƒç”¨æ–¹AIçš„è¯­ä¹‰ç†è§£èƒ½åŠ›è¿›è¡Œæ™ºèƒ½æœç´¢
2. å€™é€‰æ•°æ®ç»„ç»‡ - å°†å¯¹è¯æŒ‰é‡è¦æ€§ã€æ—¶é—´ã€æ ‡ç­¾ç­‰ç»´åº¦åˆ†ç±»ç»„ç»‡
3. æ™ºèƒ½æŒ‡ä»¤ç”Ÿæˆ - ä¸ºAIç”Ÿæˆæ¸…æ™°çš„æœç´¢ä»»åŠ¡æŒ‡ä»¤
4. é«˜æ•ˆæ•°æ®å¤„ç† - åªåŠ è½½å¿…è¦çš„å…ƒæ•°æ®ï¼Œé¿å…æ€§èƒ½é—®é¢˜

æŠ€æœ¯ç‰¹æ€§ï¼š
- æ— éœ€å¤æ‚çš„åˆ†è¯å’Œç´¢å¼•ç³»ç»Ÿ
- æ”¯æŒè‡ªç„¶è¯­è¨€æŸ¥è¯¢
- ç†è§£åŒä¹‰è¯å’Œè¯­ä¹‰ç›¸å…³æ€§
- æ™ºèƒ½å€™é€‰æ•°é‡æ§åˆ¶
- å®Œæ•´çš„é”™è¯¯å¤„ç†å’Œæ—¥å¿—è®°å½•
"""

import logging
import time
from datetime import datetime, timedelta
from typing import Dict, List, Optional, Any, Union
from dataclasses import dataclass

from synapse.storage.paths import StoragePaths
from synapse.storage.file_manager import FileManager
from synapse.models.conversation import ConversationRecord

# é…ç½®æ—¥å¿—
logger = logging.getLogger(__name__)


@dataclass
class ConversationMetadata:
    """å¯¹è¯å…ƒæ•°æ®ç»“æ„"""
    id: str
    title: str
    summary: str
    tags: List[str]
    category: str
    importance: int
    created_at: str
    updated_at: str
    content_length: int
    solutions_count: int


class SearchKnowledgeTool:
    """
    AIè¯­ä¹‰æœç´¢å·¥å…·
    
    åˆ©ç”¨è°ƒç”¨æ–¹AIçš„è¯­ä¹‰ç†è§£èƒ½åŠ›è¿›è¡Œæ™ºèƒ½æœç´¢ï¼Œå®Œå…¨æ›¿æ¢ä¼ ç»Ÿçš„å…³é”®è¯åŒ¹é…ã€‚
    
    å·¥ä½œæµç¨‹ï¼š
    1. è·å–æ‰€æœ‰å¯¹è¯åˆ—è¡¨
    2. åº”ç”¨åŸºç¡€è¿‡æ»¤æ¡ä»¶ï¼ˆæ—¶é—´ã€åˆ†ç±»ã€é‡è¦æ€§ç­‰ï¼‰
    3. æŒ‰ä¼˜å…ˆçº§åˆ†ç±»ç»„ç»‡å€™é€‰æ•°æ®
    4. ç”ŸæˆAIä»»åŠ¡æŒ‡ä»¤
    5. è¿”å›ç»“æ„åŒ–æ•°æ®ä¾›AIè¿›è¡Œè¯­ä¹‰åŒ¹é…
    
    ä½¿ç”¨ç¤ºä¾‹ï¼š
        storage_paths = StoragePaths()
        file_manager = FileManager(storage_paths)
        search_tool = SearchKnowledgeTool(storage_paths, file_manager)
        
        result = search_tool.search_knowledge(
            query="Reactæ€§èƒ½ä¼˜åŒ–",
            limit=10
        )
    """
    
    def __init__(self, storage_paths: StoragePaths, file_manager: FileManager):
        """
        åˆå§‹åŒ–AIè¯­ä¹‰æœç´¢å·¥å…·
        
        Args:
            storage_paths: å­˜å‚¨è·¯å¾„ç®¡ç†å™¨
            file_manager: æ–‡ä»¶ç®¡ç†å™¨ï¼Œç”¨äºåŠ è½½å¯¹è¯å†…å®¹
        """
        self.storage_paths = storage_paths
        self.file_manager = file_manager
        
        # é…ç½®å‚æ•°
        self.max_candidates = 100  # æœ€å¤§å€™é€‰æ•°é‡
        self.recent_days_threshold = 30  # æœ€è¿‘å¯¹è¯çš„å¤©æ•°é˜ˆå€¼
        self.high_importance_threshold = 4  # é«˜é‡è¦æ€§é˜ˆå€¼
        
        # ç¼“å­˜é…ç½®
        self.cache_ttl = 300  # ç¼“å­˜ç”Ÿå­˜æ—¶é—´5åˆ†é’Ÿ
        self._conversation_cache = {}  # å¯¹è¯åˆ—è¡¨ç¼“å­˜
        self._metadata_cache = {}  # å¯¹è¯å…ƒæ•°æ®ç¼“å­˜
        self._search_result_cache = {}  # æœç´¢ç»“æœç¼“å­˜
        
        logger.info("AIè¯­ä¹‰æœç´¢å·¥å…·åˆå§‹åŒ–å®Œæˆï¼ˆå·²å¯ç”¨ç¼“å­˜ï¼‰")
    
    def search_knowledge(
        self,
        query: str,
        category: Optional[str] = None,
        tags: Optional[List[str]] = None,
        time_range: str = "all",
        importance_min: Optional[int] = None,
        limit: int = 10,
        include_content: bool = False
    ) -> Dict[str, Any]:
        """
        æ‰§è¡ŒAIè¯­ä¹‰æœç´¢
        
        Args:
            query: æœç´¢æŸ¥è¯¢ï¼ˆå¿…éœ€ï¼‰
            category: å†…å®¹åˆ†ç±»è¿‡æ»¤ï¼ˆå¯é€‰ï¼‰
            tags: æ ‡ç­¾è¿‡æ»¤åˆ—è¡¨ï¼ˆå¯é€‰ï¼‰
            time_range: æ—¶é—´èŒƒå›´è¿‡æ»¤ ("week", "month", "all")
            importance_min: æœ€å°é‡è¦æ€§ç­‰çº§ (1-5)
            limit: æœŸæœ›è¿”å›çš„ç»“æœæ•°é‡
            include_content: æ˜¯å¦åœ¨å€™é€‰ä¸­åŒ…å«å†…å®¹é¢„è§ˆ
            
        Returns:
            Dict[str, Any]: AIè¯­ä¹‰æœç´¢çš„ç»“æ„åŒ–æ•°æ®
        """
        start_time = time.time()
        
        try:
            logger.info(f"å¼€å§‹AIè¯­ä¹‰æœç´¢: '{query}'")
            
            # 1. å‚æ•°éªŒè¯
            self._validate_search_params(query, limit)
            
            # 2. è·å–æ‰€æœ‰å¯¹è¯åˆ—è¡¨ï¼ˆä½¿ç”¨ç¼“å­˜ï¼‰
            all_conversations = self._get_cached_conversations()
            if all_conversations is None:
                all_conversations = self.file_manager.list_conversations()
                self._cache_conversations(all_conversations)
                logger.debug(f"è·å–åˆ° {len(all_conversations)} ä¸ªå¯¹è¯ï¼ˆå·²ç¼“å­˜ï¼‰")
            else:
                logger.debug(f"è·å–åˆ° {len(all_conversations)} ä¸ªå¯¹è¯ï¼ˆç¼“å­˜å‘½ä¸­ï¼‰")
            
            # 3. åº”ç”¨åŸºç¡€è¿‡æ»¤æ¡ä»¶
            filtered_conversations = self._apply_basic_filters(
                all_conversations, category, tags, time_range, importance_min
            )
            logger.debug(f"è¿‡æ»¤åå‰©ä½™ {len(filtered_conversations)} ä¸ªå¯¹è¯")
            
            # 4. é™åˆ¶å€™é€‰æ•°é‡
            limited_conversations = self._limit_candidates(filtered_conversations)
            logger.debug(f"é™åˆ¶åä¿ç•™ {len(limited_conversations)} ä¸ªå€™é€‰")
            
            # 5. ç»„ç»‡å€™é€‰æ•°æ®
            candidate_categories = self._organize_candidates(
                limited_conversations, include_content
            )
            
            # 6. ç”ŸæˆAIä»»åŠ¡æŒ‡ä»¤
            ai_instruction = self._generate_ai_instruction(query, candidate_categories, limit)
            
            # 7. è®¡ç®—å¤„ç†æ—¶é—´
            processing_time = (time.time() - start_time) * 1000
            
            # 8. æ„å»ºè¿”å›ç»“æœ
            result = {
                "search_mode": "ai_semantic",
                "query": query,
                "total_candidates": len(limited_conversations),
                "candidates_before_filter": len(all_conversations),
                "candidate_categories": candidate_categories,
                "ai_task": ai_instruction,
                "filters_applied": {
                    "category": category,
                    "tags": tags,
                    "time_range": time_range,
                    "importance_min": importance_min,
                    "limit": limit
                },
                "metadata": {
                    "processing_time_ms": round(processing_time, 2),
                    "include_content_preview": include_content,
                    "max_candidates_limit": self.max_candidates
                }
            }
            
            logger.info(f"AIè¯­ä¹‰æœç´¢å®Œæˆ: {len(limited_conversations)} ä¸ªå€™é€‰ï¼Œè€—æ—¶ {processing_time:.2f}ms")
            return result
            
        except Exception as e:
            error_msg = f"AIè¯­ä¹‰æœç´¢å¤±è´¥: {str(e)}"
            logger.error(error_msg, exc_info=True)
            raise RuntimeError(error_msg) from e
    
    def _validate_search_params(self, query: str, limit: int) -> None:
        """éªŒè¯æœç´¢å‚æ•°"""
        if not query or not query.strip():
            raise ValueError("æœç´¢æŸ¥è¯¢ä¸èƒ½ä¸ºç©º")
        
        if len(query.strip()) < 2:
            raise ValueError("æœç´¢æŸ¥è¯¢è‡³å°‘éœ€è¦2ä¸ªå­—ç¬¦")
        
        if limit < 1 or limit > 50:
            raise ValueError("ç»“æœæ•°é‡é™åˆ¶å¿…é¡»åœ¨1-50ä¹‹é—´")
    
    def _apply_basic_filters(
        self,
        conversations: List[str],
        category: Optional[str],
        tags: Optional[List[str]], 
        time_range: str,
        importance_min: Optional[int]
    ) -> List[str]:
        """
        åº”ç”¨åŸºç¡€è¿‡æ»¤æ¡ä»¶
        
        Args:
            conversations: æ‰€æœ‰å¯¹è¯IDåˆ—è¡¨
            category: åˆ†ç±»è¿‡æ»¤
            tags: æ ‡ç­¾è¿‡æ»¤
            time_range: æ—¶é—´èŒƒå›´è¿‡æ»¤
            importance_min: æœ€å°é‡è¦æ€§è¿‡æ»¤
            
        Returns:
            List[str]: è¿‡æ»¤åçš„å¯¹è¯IDåˆ—è¡¨
        """
        filtered = []
        
        for conv_id in conversations:
            try:
                # åŠ è½½å¯¹è¯å…ƒæ•°æ®ï¼ˆä½¿ç”¨ç¼“å­˜ï¼‰
                cached_metadata = self._get_cached_metadata(conv_id)
                if cached_metadata:
                    conversation = cached_metadata
                else:
                    conversation = self.file_manager.load_conversation(conv_id)
                    if not conversation:
                        continue
                    # ç¼“å­˜å…ƒæ•°æ®  
                    metadata_dict = {
                        'category': conversation.category,
                        'importance': conversation.importance,
                        'tags': conversation.tags,
                        'created_at': conversation.created_at
                    }
                    self._cache_metadata(conv_id, metadata_dict)
                    conversation = metadata_dict
                
                # åˆ†ç±»è¿‡æ»¤
                if category and conversation['category'] != category:
                    continue
                
                # é‡è¦æ€§è¿‡æ»¤
                if importance_min and conversation['importance'] < importance_min:
                    continue
                
                # æ ‡ç­¾è¿‡æ»¤
                if tags and not any(tag.lower() in [t.lower() for t in conversation['tags']] for tag in tags):
                    continue
                
                # æ—¶é—´èŒƒå›´è¿‡æ»¤
                if not self._matches_time_range(conversation['created_at'], time_range):
                    continue
                
                filtered.append(conv_id)
                
            except Exception as e:
                logger.warning(f"å¤„ç†å¯¹è¯ {conv_id} æ—¶å‡ºé”™: {e}")
                continue
        
        return filtered
    
    def _matches_time_range(self, created_at: datetime, time_range: str) -> bool:
        """æ£€æŸ¥å¯¹è¯æ˜¯å¦åœ¨æŒ‡å®šæ—¶é—´èŒƒå›´å†…"""
        if time_range == "all":
            return True
        
        try:
            now = datetime.now()
            days_diff = (now - created_at).days
            
            if time_range == "week":
                return days_diff <= 7
            elif time_range == "month":
                return days_diff <= 30
            else:
                return True
                
        except Exception:
            return True
    
    def _limit_candidates(self, conversations: List[str]) -> List[str]:
        """
        é™åˆ¶å€™é€‰æ•°é‡ï¼Œé¿å…AIè¾“å…¥è¿‡é•¿
        
        ä¼˜å…ˆçº§æ’åºï¼šé‡è¦æ€§é«˜ > æ—¶é—´æ–° > å…¶ä»–
        """
        if len(conversations) <= self.max_candidates:
            return conversations
        
        # åŠ è½½å…ƒæ•°æ®è¿›è¡Œä¼˜å…ˆçº§æ’åº
        conversation_priorities = []
        for conv_id in conversations:
            try:
                conversation = self.file_manager.load_conversation(conv_id)
                if conversation:
                    # è®¡ç®—ä¼˜å…ˆçº§åˆ†æ•°
                    priority_score = self._calculate_priority_score(conversation)
                    conversation_priorities.append((conv_id, priority_score))
            except Exception as e:
                logger.warning(f"è®¡ç®—ä¼˜å…ˆçº§æ—¶å¤„ç†å¯¹è¯ {conv_id} å¤±è´¥: {e}")
                continue
        
        # æŒ‰ä¼˜å…ˆçº§æ’åºå¹¶å–å‰Nä¸ª
        conversation_priorities.sort(key=lambda x: x[1], reverse=True)
        return [conv_id for conv_id, _ in conversation_priorities[:self.max_candidates]]
    
    def _calculate_priority_score(self, conversation: ConversationRecord) -> float:
        """
        è®¡ç®—å¯¹è¯çš„ä¼˜å…ˆçº§åˆ†æ•°
        
        è€ƒè™‘å› ç´ ï¼š
        - é‡è¦æ€§ (40%æƒé‡)
        - æ—¶é—´æ–°é²œåº¦ (30%æƒé‡)
        - å†…å®¹ä¸°å¯Œåº¦ (20%æƒé‡)
        - è§£å†³æ–¹æ¡ˆæ•°é‡ (10%æƒé‡)
        """
        score = 0.0
        
        # é‡è¦æ€§åˆ†æ•° (0-1)
        importance_score = conversation.importance / 5.0
        score += importance_score * 0.4
        
        # æ—¶é—´æ–°é²œåº¦åˆ†æ•° (0-1)
        try:
            days_old = (datetime.now() - conversation.created_at).days
            freshness_score = max(0, (365 - days_old) / 365)  # ä¸€å¹´å†…çš„å¯¹è¯
            score += freshness_score * 0.3
        except:
            score += 0.1  # é»˜è®¤åˆ†æ•°
        
        # å†…å®¹ä¸°å¯Œåº¦åˆ†æ•° (0-1)
        content_length = len(conversation.content)
        richness_score = min(1.0, content_length / 1000)  # 1000å­—ç¬¦ä¸ºæ»¡åˆ†
        score += richness_score * 0.2
        
        # è§£å†³æ–¹æ¡ˆæ•°é‡åˆ†æ•° (0-1)
        solutions_score = min(1.0, len(conversation.solutions) / 5)  # 5ä¸ªè§£å†³æ–¹æ¡ˆä¸ºæ»¡åˆ†
        score += solutions_score * 0.1
        
        return score
    
    def _organize_candidates(
        self,
        conversation_ids: List[str], 
        include_content: bool = False
    ) -> Dict[str, List[Dict]]:
        """
        å°†å€™é€‰å¯¹è¯æŒ‰ç±»å‹åˆ†ç±»ç»„ç»‡
        
        åˆ†ç±»ç»´åº¦ï¼š
        - high_importance: é«˜é‡è¦æ€§å¯¹è¯ (é‡è¦æ€§>=4)
        - recent_discussions: æœ€è¿‘å¯¹è¯ (30å¤©å†…)
        - tagged_content: æœ‰æ ‡ç­¾çš„å¯¹è¯
        - general_conversations: ä¸€èˆ¬å¯¹è¯
        """
        categories = {
            "high_importance": [],
            "recent_discussions": [],
            "tagged_content": [],
            "general_conversations": []
        }
        
        for conv_id in conversation_ids:
            try:
                conversation = self.file_manager.load_conversation(conv_id)
                if not conversation:
                    continue
                
                # æ„å»ºå€™é€‰ä¿¡æ¯
                candidate_info = self._build_candidate_info(conversation, include_content)
                
                # åˆ†ç±»é€»è¾‘
                if conversation.importance >= self.high_importance_threshold:
                    categories["high_importance"].append(candidate_info)
                elif self._is_recent_conversation(conversation.created_at):
                    categories["recent_discussions"].append(candidate_info)
                elif conversation.tags and any(tag.strip() for tag in conversation.tags):
                    categories["tagged_content"].append(candidate_info)
                else:
                    categories["general_conversations"].append(candidate_info)
                    
            except Exception as e:
                logger.warning(f"ç»„ç»‡å€™é€‰æ•°æ®æ—¶å¤„ç†å¯¹è¯ {conv_id} å¤±è´¥: {e}")
                continue
        
        return categories
    
    def _build_candidate_info(
        self, 
        conversation: ConversationRecord, 
        include_content: bool = False
    ) -> Dict[str, Any]:
        """æ„å»ºå€™é€‰å¯¹è¯çš„ä¿¡æ¯å­—å…¸"""
        info = {
            "id": conversation.id,
            "title": conversation.title,
            "summary": conversation.summary,
            "tags": conversation.tags,
            "category": conversation.category,
            "importance": conversation.importance,
            "created_at": conversation.created_at.isoformat(),
            "content_length": len(conversation.content),
            "solutions_count": len(conversation.solutions)
        }
        
        if include_content:
            # åªåŒ…å«å†…å®¹çš„å‰300ä¸ªå­—ç¬¦ä½œä¸ºé¢„è§ˆ
            content_preview = conversation.content[:300]
            if len(conversation.content) > 300:
                content_preview += "..."
            info["content_preview"] = content_preview
        
        return info
    
    def _is_recent_conversation(self, created_at: datetime) -> bool:
        """åˆ¤æ–­æ˜¯å¦ä¸ºæœ€è¿‘çš„å¯¹è¯"""
        try:
            days_old = (datetime.now() - created_at).days
            return days_old <= self.recent_days_threshold
        except:
            return False
    
    def _generate_ai_instruction(
        self, 
        query: str, 
        categories: Dict[str, List[Dict]], 
        limit: int
    ) -> str:
        """
        ä¸ºAIç”Ÿæˆæ¸…æ™°çš„æœç´¢ä»»åŠ¡æŒ‡ä»¤
        """
        total_candidates = sum(len(cat) for cat in categories.values())
        
        instruction = f"""è¯·æ ¹æ®æŸ¥è¯¢ "{query}" ä»ä»¥ä¸‹ {total_candidates} ä¸ªå€™é€‰å¯¹è¯ä¸­è¿›è¡Œè¯­ä¹‰åŒ¹é…å’Œç­›é€‰ï¼š

ğŸŒŸ **é«˜é‡è¦æ€§å¯¹è¯** ({len(categories['high_importance'])}ä¸ª)
é‡è¦æ€§è¯„åˆ† â‰¥ 4 çš„å¯¹è¯ï¼Œé€šå¸¸åŒ…å«é‡è¦çš„æŠ€æœ¯è§£å†³æ–¹æ¡ˆå’Œæ·±åº¦è®¨è®ºï¼š
{self._format_candidates_for_display(categories['high_importance'])}

ğŸ•’ **æœ€è¿‘è®¨è®º** ({len(categories['recent_discussions'])}ä¸ª)  
æœ€è¿‘30å¤©å†…çš„å¯¹è¯ï¼Œå¯èƒ½åŒ…å«æœ€æ–°çš„æŠ€æœ¯è¶‹åŠ¿å’Œé—®é¢˜ï¼š
{self._format_candidates_for_display(categories['recent_discussions'])}

ğŸ·ï¸ **æ ‡ç­¾ç›¸å…³** ({len(categories['tagged_content'])}ä¸ª)
å¸¦æœ‰æŠ€æœ¯æ ‡ç­¾çš„å¯¹è¯ï¼Œä¾¿äºæŠ€æœ¯ä¸»é¢˜åŒ¹é…ï¼š
{self._format_candidates_for_display(categories['tagged_content'])}

ğŸ“ **ä¸€èˆ¬å¯¹è¯** ({len(categories['general_conversations'])}ä¸ª)
å…¶ä»–ä¸€èˆ¬å¯¹è¯ï¼š
{self._format_candidates_for_display(categories['general_conversations'])}

**ä»»åŠ¡è¦æ±‚ï¼š**
1. æ ¹æ®è¯­ä¹‰ç›¸å…³æ€§é€‰æ‹©æœ€ç¬¦åˆæŸ¥è¯¢ "{query}" çš„ {limit} ä¸ªå¯¹è¯
2. ä¼˜å…ˆè€ƒè™‘ï¼š
   - æ ‡é¢˜å’Œå†…å®¹ä¸æŸ¥è¯¢è¯­ä¹‰é«˜åº¦ç›¸å…³çš„å¯¹è¯
   - é«˜é‡è¦æ€§ä¸”ä¸»é¢˜ç›¸å…³çš„å¯¹è¯
   - æŠ€æœ¯æ ‡ç­¾åŒ¹é…çš„ä¸“ä¸šè®¨è®º
   - æœ€è¿‘çš„ç›¸å…³è®¨è®º
3. è¯·ç›´æ¥è¿”å›é€‰ä¸­å¯¹è¯çš„å®Œæ•´ä¿¡æ¯ï¼ŒæŒ‰ç›¸å…³æ€§æ’åº
4. ç®€è¦è§£é‡Šæ¯ä¸ªé€‰æ‹©çš„ç†ç”±

è¯·å¼€å§‹è¯­ä¹‰åŒ¹é…å’Œç­›é€‰ã€‚"""
        
        return instruction
    
    def _format_candidates_for_display(self, candidates: List[Dict]) -> str:
        """å°†å€™é€‰å¯¹è¯æ ¼å¼åŒ–ä¸ºæ˜“è¯»çš„æ˜¾ç¤ºæ ¼å¼"""
        if not candidates:
            return "  (æ— )"
        
        formatted = []
        for i, candidate in enumerate(candidates, 1):
            tags_str = ", ".join(candidate["tags"]) if candidate["tags"] else "æ— æ ‡ç­¾"
            
            formatted.append(f"""  {i}. ã€{candidate['title']}ã€‘
     ID: {candidate['id']}
     æ‘˜è¦: {candidate['summary'][:100]}{'...' if len(candidate['summary']) > 100 else ''}
     æ ‡ç­¾: {tags_str}
     é‡è¦æ€§: {candidate['importance']}/5
     æ—¶é—´: {candidate['created_at'][:10]}""")
        
        return "\n".join(formatted)
    
    def get_search_statistics(self) -> Dict[str, Any]:
        """è·å–æœç´¢å·¥å…·ç»Ÿè®¡ä¿¡æ¯"""
        try:
            all_conversations = self.file_manager.list_conversations()
            
            return {
                "search_mode": "ai_semantic",
                "total_conversations": len(all_conversations),
                "max_candidates_limit": self.max_candidates,
                "recent_days_threshold": self.recent_days_threshold,
                "high_importance_threshold": self.high_importance_threshold,
                "last_updated": datetime.now().isoformat()
            }
            
        except Exception as e:
            logger.error(f"è·å–æœç´¢ç»Ÿè®¡ä¿¡æ¯å¤±è´¥: {e}")
            return {
                "search_mode": "ai_semantic",
                "error": str(e)
            }
    
    def _generate_cache_key(self, query: str, category: Optional[str], tags: Optional[List[str]], 
                           time_range: str, importance_min: Optional[int], limit: int, 
                           include_content: bool) -> str:
        """ç”Ÿæˆæœç´¢ç¼“å­˜é”®"""
        import hashlib
        
        components = [
            query.strip().lower(),
            category or "",
            ",".join(sorted(tags or [])),
            time_range,
            str(importance_min or 0),
            str(limit),
            str(include_content)
        ]
        
        cache_key = hashlib.md5("|".join(components).encode()).hexdigest()
        return f"search_{cache_key}"
    
    def _get_cached_conversations(self) -> Optional[List[str]]:
        """è·å–ç¼“å­˜çš„å¯¹è¯åˆ—è¡¨"""
        cache_key = "conversations_list"
        if cache_key not in self._conversation_cache:
            return None
        
        cached_item = self._conversation_cache[cache_key]
        cached_time = cached_item.get("cached_at", 0)
        
        # æ£€æŸ¥ç¼“å­˜æ˜¯å¦è¿‡æœŸ
        if time.time() - cached_time > self.cache_ttl:
            del self._conversation_cache[cache_key]
            return None
        
        return cached_item["conversations"]
    
    def _cache_conversations(self, conversations: List[str]) -> None:
        """ç¼“å­˜å¯¹è¯åˆ—è¡¨"""
        cache_key = "conversations_list"
        self._conversation_cache[cache_key] = {
            "conversations": conversations,
            "cached_at": time.time()
        }
    
    def _get_cached_metadata(self, conv_id: str) -> Optional[Dict]:
        """è·å–ç¼“å­˜çš„å¯¹è¯å…ƒæ•°æ®"""
        if conv_id not in self._metadata_cache:
            return None
        
        cached_item = self._metadata_cache[conv_id]
        cached_time = cached_item.get("cached_at", 0)
        
        # æ£€æŸ¥ç¼“å­˜æ˜¯å¦è¿‡æœŸ
        if time.time() - cached_time > self.cache_ttl:
            del self._metadata_cache[conv_id]
            return None
        
        return cached_item["metadata"]
    
    def _cache_metadata(self, conv_id: str, metadata: Dict) -> None:
        """ç¼“å­˜å¯¹è¯å…ƒæ•°æ®"""
        self._metadata_cache[conv_id] = {
            "metadata": metadata,
            "cached_at": time.time()
        }
        
        # ç®€å•çš„ç¼“å­˜å¤§å°æ§åˆ¶
        if len(self._metadata_cache) > 200:
            # æ¸…ç†æœ€æ—§çš„50ä¸ªç¼“å­˜é¡¹
            sorted_items = sorted(
                self._metadata_cache.items(),
                key=lambda x: x[1].get("cached_at", 0)
            )
            for key, _ in sorted_items[:50]:
                del self._metadata_cache[key]
    
    def clear_cache(self) -> Dict[str, int]:
        """æ¸…ç©ºæ‰€æœ‰ç¼“å­˜"""
        stats = {
            "conversations_cache": len(self._conversation_cache),
            "metadata_cache": len(self._metadata_cache), 
            "search_result_cache": len(self._search_result_cache)
        }
        
        self._conversation_cache.clear()
        self._metadata_cache.clear()
        self._search_result_cache.clear()
        
        logger.info(f"å·²æ¸…ç©ºç¼“å­˜: {stats}")
        return stats