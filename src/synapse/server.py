"""
Synapse MCP Server - Main Server Module

This module contains the complete implementation of an MCP (Model Context Protocol) server
that provides intelligent memory and knowledge retrieval functionality.

Key Features:
- FastMCP framework-based MCP server
- Tool registration (save-conversation, search-knowledge, inject-context)
- Unified error handling and response formatting
- Parameter validation and server lifecycle management
- MCP protocol compliance for seamless AI assistant integration
"""

import asyncio
import logging
import json
import time
from contextlib import asynccontextmanager
from dataclasses import dataclass
from datetime import datetime
import sys

from mcp.server.fastmcp import FastMCP, Context

from synapse.models import ConversationRecord
from synapse.storage.paths import StoragePaths
from synapse.storage.initializer import StorageInitializer, initialize_synapse_storage
from synapse.storage.file_manager import FileManager
from synapse.utils.logging_config import setup_logging
from synapse.tools.save_conversation import SaveConversationTool
from synapse.tools.search_knowledge import SearchKnowledgeTool
from synapse.tools.inject_context import InjectContextTool
from synapse.tools.extract_solutions import ExtractSolutionsTool

logger = logging.getLogger(__name__)

# Mock Database class for demonstration purposes
class Database:
    """Mock database class for demonstration purposes"""
    
    @classmethod
    async def connect(cls) -> "Database":
        """Connect to database"""
        logger.info("Database connection established")
        return cls()
    
    async def disconnect(self) -> None:
        """Disconnect from database"""
        logger.info("Database connection closed")
    
    async def save_conversation(self, conversation: ConversationRecord) -> str:
        """Save conversation record (mock implementation)"""
        logger.info(f"Saving conversation: {conversation.id}")
        return conversation.id
    
    async def search_conversations(self, query: str, limit: int = 10) -> list:
        """Search conversation records (mock implementation)"""
        logger.info(f"Search query: '{query}', limit: {limit}")
        return [
            {
                "id": "conv_20240115_001",
                "title": f"Solution for '{query}'",
                "snippet": f"This is a detailed explanation and code example for {query}...",
                "match_score": 0.95,
                "created_at": "2024-01-15T10:30:00Z",
                "tags": [query.lower(), "programming"],
                "type": "conversation"
            }
        ]

@dataclass
class AppContext:
    """Application context containing database connections and resources"""
    db: Database
    storage_paths: StoragePaths
    file_manager: FileManager
    save_conversation_tool: SaveConversationTool
    search_knowledge_tool: SearchKnowledgeTool
    inject_context_tool: InjectContextTool
    extract_solutions_tool: ExtractSolutionsTool

@asynccontextmanager
async def app_lifespan(_: FastMCP):
    """Manage application lifecycle with resource initialization and cleanup"""
    logger.info("Starting Synapse MCP Server...")
    
    try:
        # Initialize storage system
        loop = asyncio.get_event_loop()
        await loop.run_in_executor(None, initialize_synapse_storage)
        
        # Connect to database
        db = await Database.connect()
        
        # Create resource managers
        storage_paths = StoragePaths()
        file_manager = FileManager(storage_paths)
        
        # Create tool instances
        save_conversation_tool = SaveConversationTool(storage_paths)
        search_knowledge_tool = SearchKnowledgeTool(storage_paths, file_manager)
        inject_context_tool = InjectContextTool(storage_paths, file_manager, search_knowledge_tool)
        extract_solutions_tool = ExtractSolutionsTool(storage_paths)
        
        # Create application context
        app_context = AppContext(
            db=db, 
            storage_paths=storage_paths,
            file_manager=file_manager,
            save_conversation_tool=save_conversation_tool,
            search_knowledge_tool=search_knowledge_tool,
            inject_context_tool=inject_context_tool,
            extract_solutions_tool=extract_solutions_tool
        )
        
        logger.info("Synapse MCP Server started successfully")
        yield app_context
        
    except Exception as e:
        logger.error(f"Server startup failed: {str(e)}")
        raise
    finally:
        # Cleanup resources
        logger.info("Shutting down Synapse MCP Server...")
        if 'db' in locals():
            await db.disconnect()
        logger.info("Synapse MCP Server shutdown complete")

# Create FastMCP server instance
mcp = FastMCP("synapse-mcp", lifespan=app_lifespan)

# ==================== MCP Prompt Templates ====================

@mcp.prompt(title="Conversation Analysis")
def conversation_analysis_prompt(
    title: str,
    focus: str = "comprehensive"
) -> str:
    """
    Conversation analysis prompt template.
    
    Analyzes the current conversation context automatically without requiring
    explicit content input. The AI will analyze the full conversation history.
    
    Args:
        title: Conversation title for the analysis
        focus: Analysis focus ("comprehensive", "summary", "tags", "solutions")
    
    Returns:
        str: Formatted analysis prompt for current conversation context
    """
    base_prompt = f"""Please analyze the current technical conversation and provide structured analysis results.

## Conversation Information
**Title**: {title}
**Content**: Please analyze the complete current conversation context

## Analysis Requirements
Please provide the following analysis results in JSON format:

```json
{{
    "summary": "Concise summary of the current conversation (1-2 sentences)",
    "tags": ["technical_tag1", "technical_tag2", "framework_name", "..."],
    "importance": 3,
    "category": "problem-solving | learning | code-review | implementation | debugging | general",
    "has_solutions": true,
    "solution_indicators": ["Types of solutions or approaches discussed"]
}}
```

## Analysis Focus"""
    
    if focus == "comprehensive":
        return base_prompt + """
- Comprehensive analysis: summary, tags, importance, category, solution identification
- Identify tech stack, programming languages, frameworks
- Evaluate educational value and practical utility
- Detect reusable solutions"""
    elif focus == "summary":
        return base_prompt + """
- Focus on generating accurate and concise summaries
- Extract core technical points and conclusions"""
    elif focus == "tags":
        return base_prompt + """
- Focus on extracting technical tags
- Identify programming languages, frameworks, tools, concepts
- Sort by relevance, maximum 10 tags"""
    elif focus == "solutions":
        return base_prompt + """
- Focus on identifying reusable solutions
- Detect code snippets, configuration examples, best practices
- Evaluate solution generalizability"""
    else:
        return base_prompt

# ==================== MCP Tool Implementations ====================

@mcp.tool()
async def save_conversation(
    title: str,
    tags: list[str] = None,
    category: str = None, 
    importance: int = None,
    check_duplicates: bool = True,
    # AIåˆ†æç»“æœå‚æ•°ï¼ˆé€šè¿‡conversation_analysis_promptè·å¾—åä¼ å…¥ï¼‰
    ai_summary: str = None,
    ai_tags: list[str] = None,
    ai_importance: int = None,
    ai_category: str = None,
    ai_solutions: list[dict] = None,
    ctx: Context = None
) -> dict:
    """
    Save AI conversation records to knowledge base based on current conversation context.
    
    This tool works with AI analysis results to store conversation metadata:
    - Accept user-specified or AI-analyzed metadata
    - Extract content from current conversation context automatically
    - Detect and notify duplicate conversations
    - Update search indexes for fast queries
    
    Recommended usage workflow:
    1. User says: "å¸®æˆ‘ä¿å­˜ä»Šå¤©çš„å¯¹è¯"
    2. System calls conversation_analysis_prompt to analyze current context
    3. System passes AI analysis results to this tool for saving
    
    Args:
        title: Conversation topic title (required)
        tags: User-defined tag list (optional)
        category: User-specified conversation category (optional)
        importance: User-specified importance level 1-5 (optional)
        check_duplicates: Whether to check for duplicate conversations (default True)
        ai_summary: AI-generated summary from conversation_analysis_prompt
        ai_tags: AI-extracted tag list from conversation_analysis_prompt
        ai_importance: AI-assessed importance from conversation_analysis_prompt
        ai_category: AI-inferred category from conversation_analysis_prompt
        ai_solutions: AI-extracted solution list from conversation_analysis_prompt
        ctx: MCP context object for progress reporting
        
    Returns:
        dict: Detailed save result information including:
            - success: Whether save was successful
            - conversation: Saved conversation details
            - duplicates_found: Number of duplicate conversations found
            - duplicate_ids: List of duplicate conversation IDs
            - storage_path: File storage path
    """
    try:
        if ctx:
            await ctx.info(f"Starting conversation save: {title}")
        
        # Basic parameter validation
        if not title or not title.strip():
            raise ValueError("Conversation title cannot be empty")
        
        if not ai_summary:
            raise ValueError("AI analysis results are required. Please first call conversation_analysis_prompt to analyze the current conversation.")
        
        if importance is not None and (importance < 1 or importance > 5):
            raise ValueError("Importance level must be between 1-5")
        
        # Get save conversation tool instance
        save_tool = None
        if ctx and hasattr(ctx, 'request_context') and hasattr(ctx.request_context, 'lifespan_context'):
            save_tool = ctx.request_context.lifespan_context.save_conversation_tool
        
        if not save_tool:
            raise RuntimeError("æ— æ³•è·å–SaveConversationToolå®ä¾‹ï¼Œè¯·æ£€æŸ¥æœåŠ¡å™¨é…ç½®")
        
        if ctx:
            await ctx.info("Processing conversation with AI analysis results...")
        
        # Create conversation content from AI summary since we no longer require explicit content
        # The AI has already analyzed the complete conversation context
        conversation_content = ai_summary + "\n\n[Full conversation content was analyzed by AI system]"
        
        # ä½¿ç”¨SaveConversationToolè¿›è¡Œä¿å­˜
        result = await save_tool.save_conversation(
            title=title.strip(),
            content=conversation_content,
            user_tags=tags,
            user_category=category,
            user_importance=importance,
            check_duplicates=check_duplicates,
            # ä¼ é€’AIåˆ†æç»“æœ
            ai_summary=ai_summary,
            ai_tags=ai_tags,
            ai_importance=ai_importance,
            ai_category=ai_category,
            ai_solutions=ai_solutions,
            ctx=ctx
        )
        
        # æ£€æŸ¥ä¿å­˜ç»“æœ
        if not result.get("success", False):
            error_msg = result.get("error", "æœªçŸ¥é”™è¯¯")
            raise RuntimeError(f"ä¿å­˜å¤±è´¥: {error_msg}")
        
        conversation_info = result.get("conversation", {})
        duplicates_count = result.get("duplicates_found", 0)
        
        if ctx:
            if duplicates_count > 0:
                await ctx.info(f"æ£€æµ‹åˆ° {duplicates_count} ä¸ªç›¸ä¼¼å¯¹è¯")
            
            await ctx.info(f"å¯¹è¯ä¿å­˜æˆåŠŸ: {conversation_info.get('id', 'Unknown')}")
            await ctx.info(f"è‡ªåŠ¨æå–äº† {conversation_info.get('auto_tags_count', 0)} ä¸ªæ ‡ç­¾")
        
        # æ„å»ºè¿”å›ç»“æœï¼ˆä¿æŒä¸åŸAPIå…¼å®¹ï¼ŒåŒæ—¶æä¾›æ›´å¤šä¿¡æ¯ï¼‰
        return {
            # åŸºæœ¬ä¿¡æ¯ï¼ˆä¿æŒå…¼å®¹ï¼‰
            "id": conversation_info.get("id"),
            "title": conversation_info.get("title"),
            "stored_at": conversation_info.get("created_at"),
            "tags": conversation_info.get("tags", []),
            "category": conversation_info.get("category", "general"),
            "importance": conversation_info.get("importance", 3),
            "searchable": True,
            
            # æ‰©å±•ä¿¡æ¯
            "summary": conversation_info.get("summary", ""),
            "auto_generated": {
                "tags_count": conversation_info.get("auto_tags_count", 0),
                "user_tags_count": conversation_info.get("user_tags_count", 0),
                "solutions_found": conversation_info.get("solutions_count", 0)
            },
            "duplicate_detection": {
                "checked": check_duplicates,
                "duplicates_found": duplicates_count,
                "duplicate_ids": result.get("duplicate_ids", [])
            },
            "storage_info": {
                "file_path": result.get("storage_path"),
                "indexed": True  # ç´¢å¼•å·²æ›´æ–°
            }
        }
        
    except Exception as e:
        error_msg = str(e)
        if ctx:
            await ctx.error(f"ä¿å­˜å¯¹è¯å¤±è´¥: {error_msg}")
        
        logger.error(f"ä¿å­˜å¯¹è¯å¤±è´¥ - æ ‡é¢˜: {title[:50]}..., é”™è¯¯: {error_msg}", exc_info=True)
        
        # è¿”å›é”™è¯¯ä¿¡æ¯è€Œä¸æ˜¯æŠ›å‡ºå¼‚å¸¸ï¼Œè®©è°ƒç”¨æ–¹èƒ½å¤Ÿå¤„ç†
        return {
            "success": False,
            "error": error_msg,
            "error_type": type(e).__name__,
            "searchable": False
        }

@mcp.tool()
async def search_knowledge(
    query: str,
    search_in: str = "all",
    limit: int = 10,
    ctx: Context = None
) -> dict:
    """
    ç®€å•grepæœç´¢å·¥å…· - è®©AIæä¾›å…³é”®è¯ï¼Œå·¥å…·è´Ÿè´£æœç´¢
    
    ğŸ¤– AIä½¿ç”¨å»ºè®®ï¼š
    - æ ¹æ®ç”¨æˆ·é—®é¢˜ç”Ÿæˆå¤šä¸ªä¸åŒçš„å…³é”®è¯è¿›è¡Œæœç´¢
    - å°è¯•ä¸­è‹±æ–‡ã€æŠ€æœ¯æœ¯è¯­çš„ä¸åŒè¡¨è¾¾æ–¹å¼
    - å»ºè®®è¿ç»­æœç´¢ 2-3 æ¬¡ä¸åŒå…³é”®è¯ä»¥æé«˜å¬å›ç‡
    
    ç¤ºä¾‹ç”¨æ³•ï¼š
    ç”¨æˆ·é—®ï¼š"Pythonå¼‚æ­¥ç¼–ç¨‹é”™è¯¯å¤„ç†"
    AIåº”è¯¥æœç´¢ï¼š
    1. search_knowledge("Python async exception")
    2. search_knowledge("å¼‚æ­¥ é”™è¯¯å¤„ç†")
    3. search_knowledge("asyncio try except")
    
    Args:
        query: æœç´¢å…³é”®è¯ï¼ˆç”±AIç†è§£ç”¨æˆ·é—®é¢˜åç”Ÿæˆï¼‰ï¼ˆå¿…éœ€ï¼‰
        search_in: æœç´¢èŒƒå›´ ("title", "content", "tags", "all")
        limit: è¿”å›ç»“æœæ•°é‡ (1-50)
        ctx: MCPä¸Šä¸‹æ–‡å¯¹è±¡
        
    Returns:
        dict: æœç´¢ç»“æœå’ŒAIåç»­æœç´¢å»ºè®®
        {
            "query": "ç”¨æˆ·è¾“å…¥çš„å…³é”®è¯",
            "total_found": 5,
            "search_area": "all",
            "results": [
                {
                    "id": "sol_001",
                    "title": "...",
                    "snippet": "åŒ¹é…å†…å®¹ç‰‡æ®µ...",
                    "created_at": "2024-01-01T12:00:00Z",
                    "match_reason": "æ ‡é¢˜åŒ¹é… 'async'"
                }
            ],
            "suggestion": "å»ºè®®AIå°è¯•æœç´¢å…¶ä»–ç›¸å…³å…³é”®è¯å¦‚ 'asyncio', 'å¼‚æ­¥ç¼–ç¨‹'"
        }
    """
    try:
        if ctx:
            await ctx.info(f"å¼€å§‹AIè¯­ä¹‰æœç´¢: '{query}'")
        
        # è·å–æœç´¢çŸ¥è¯†å·¥å…·å®ä¾‹
        search_tool = None
        if ctx and hasattr(ctx, 'request_context') and hasattr(ctx.request_context, 'lifespan_context'):
            search_tool = ctx.request_context.lifespan_context.search_knowledge_tool
        
        if not search_tool:
            raise RuntimeError("æ— æ³•è·å–SearchKnowledgeToolå®ä¾‹ï¼Œè¯·æ£€æŸ¥æœåŠ¡å™¨é…ç½®")
        
        if ctx:
            await ctx.info("æ‰§è¡Œç®€å•æ–‡æœ¬æœç´¢...")
        
        # ä½¿ç”¨ç®€åŒ–çš„SearchKnowledgeToolè¿›è¡Œgrepæœç´¢
        result = search_tool.search_knowledge(
            query=query.strip(),
            search_in=search_in,
            limit=limit
        )
        
        # æ£€æŸ¥æœç´¢ç»“æœ
        if not result:
            raise RuntimeError("æœç´¢å·¥å…·è¿”å›ç©ºç»“æœ")
        
        processing_time = result.get("processing_time_ms", 0)
        total_found = result.get("total_found", 0)
        
        if ctx:
            await ctx.info(f"grepæœç´¢å®Œæˆ: æ‰¾åˆ° {total_found} ä¸ªç»“æœï¼Œè€—æ—¶ {processing_time:.2f}ms")
            if result.get("suggestion"):
                await ctx.info(f"AIå»ºè®®: {result['suggestion']}")
        
        # è¿”å›ç®€åŒ–çš„æœç´¢ç»“æœ
        return result
        
    except Exception as e:
        error_msg = str(e)
        if ctx:
            await ctx.error(f"ç®€åŒ–grepæœç´¢å¤±è´¥: {error_msg}")
        
        logger.error(f"ç®€åŒ–grepæœç´¢å¤±è´¥ - æŸ¥è¯¢: '{query}', é”™è¯¯: {error_msg}", exc_info=True)
        
        # è¿”å›é”™è¯¯ä¿¡æ¯
        return {
            "query": query,
            "total_found": 0,
            "search_area": search_in,
            "results": [],
            "error": error_msg,
            "processing_time_ms": 0
        }


@mcp.tool()
async def inject_context(
    current_query: str,
    max_items: int = 3,
    relevance_threshold: float = 0.7,
    include_solutions: bool = True,
    include_conversations: bool = True,
    ctx: Context = None
) -> dict:
    """
    å‘å½“å‰å¯¹è¯æ³¨å…¥ç›¸å…³çš„è§£å†³æ–¹æ¡ˆä¸Šä¸‹æ–‡
    
    ä½¿ç”¨æ™ºèƒ½è§£å†³æ–¹æ¡ˆæ³¨å…¥ç³»ç»Ÿï¼ŒåŸºäºå¤šå› å­ç›¸å…³æ€§ç®—æ³•ä»å·²æå–çš„è§£å†³æ–¹æ¡ˆä¸­
    æ‰¾åˆ°æœ€ç›¸å…³çš„ä»£ç ç‰‡æ®µã€æ–¹æ³•å’Œæ¨¡å¼ï¼Œä¸ºå½“å‰å¯¹è¯æä¾›æœ‰ä»·å€¼çš„æŠ€æœ¯å‚è€ƒã€‚
    
    ç›¸å…³æ€§è®¡ç®—åŸºäºä»¥ä¸‹å› ç´ ï¼š
    - æ–‡æœ¬ç›¸ä¼¼åº¦ï¼ˆ35%ï¼‰ï¼šå…³é”®è¯åŒ¹é…å’Œå†…å®¹é‡å åˆ†æ
    - æ ‡ç­¾åŒ¹é…ï¼ˆ25%ï¼‰ï¼šåŸºäºæŠ€æœ¯æ ‡ç­¾çš„ç›¸ä¼¼åº¦
    - å¼•ç”¨è®¡æ•°åŠ æˆï¼ˆ25%ï¼‰ï¼šåŸºäºè§£å†³æ–¹æ¡ˆä½¿ç”¨é¢‘ç‡çš„æƒé‡
    - å¯é‡ç”¨æ€§åŠ æˆï¼ˆ10%ï¼‰ï¼šåŸºäºè§£å†³æ–¹æ¡ˆè´¨é‡è¯„åˆ†
    - æœ€è¿‘å¼•ç”¨åŠ æˆï¼ˆ5%ï¼‰ï¼šåŸºäºæœ€è¿‘ä½¿ç”¨æ—¶é—´çš„æƒé‡
    
    Args:
        current_query: å½“å‰ç”¨æˆ·çš„æŸ¥è¯¢æˆ–é—®é¢˜ï¼ˆå¿…éœ€ï¼‰
        max_items: æœ€å¤§æ³¨å…¥çš„è§£å†³æ–¹æ¡ˆæ•°é‡ (1-10ï¼Œé»˜è®¤3)
        relevance_threshold: ç›¸å…³æ€§é˜ˆå€¼ (0.0-1.0ï¼Œé»˜è®¤0.7)
        include_solutions: æ˜¯å¦åŒ…å«è§£å†³æ–¹æ¡ˆå†…å®¹ï¼ˆä¿ç•™å…¼å®¹æ€§ï¼Œé»˜è®¤Trueï¼‰
        include_conversations: å·²åºŸå¼ƒï¼Œä¿ç•™å‘åå…¼å®¹æ€§
        ctx: MCPä¸Šä¸‹æ–‡å¯¹è±¡
        
    Returns:
        dict: åŒ…å«ä¸Šä¸‹æ–‡é¡¹ã€æ³¨å…¥æ‘˜è¦å’Œè¯¦ç»†ç»Ÿè®¡ä¿¡æ¯çš„ç»“æœ
        {
            "context_items": [...],
            "injection_summary": str,
            "total_items": int,
            "processing_time_ms": float,
            "search_statistics": {...}
        }
    """
    try:
        if ctx:
            await ctx.info(f"å¼€å§‹æ™ºèƒ½ä¸Šä¸‹æ–‡æ³¨å…¥: '{current_query[:50]}'")
        
        # åŸºç¡€å‚æ•°éªŒè¯
        if not current_query or not current_query.strip():
            raise ValueError("å½“å‰æŸ¥è¯¢ä¸èƒ½ä¸ºç©º")
        
        if not isinstance(max_items, int) or max_items < 1 or max_items > 10:
            raise ValueError("æœ€å¤§æ³¨å…¥é¡¹æ•°å¿…é¡»åœ¨1-10ä¹‹é—´")
        
        if not isinstance(relevance_threshold, (int, float)) or relevance_threshold < 0.0 or relevance_threshold > 1.0:
            raise ValueError("ç›¸å…³æ€§é˜ˆå€¼å¿…é¡»åœ¨0.0-1.0ä¹‹é—´")
        
        # è·å–ä¸Šä¸‹æ–‡æ³¨å…¥å·¥å…·å®ä¾‹
        inject_tool = None
        if ctx and hasattr(ctx, 'request_context') and hasattr(ctx.request_context, 'lifespan_context'):
            inject_tool = ctx.request_context.lifespan_context.inject_context_tool
        
        if not inject_tool:
            raise RuntimeError("æ— æ³•è·å–InjectContextToolå®ä¾‹ï¼Œè¯·æ£€æŸ¥æœåŠ¡å™¨é…ç½®")
        
        if ctx:
            await ctx.info("æ‰§è¡Œæ™ºèƒ½ç›¸å…³æ€§åˆ†æ...")
            if relevance_threshold > 0.8:
                await ctx.info(f"ä½¿ç”¨é«˜è´¨é‡é˜ˆå€¼: {relevance_threshold}")
        
        # ä½¿ç”¨InjectContextToolè¿›è¡Œæ™ºèƒ½ä¸Šä¸‹æ–‡æ³¨å…¥
        result = inject_tool.inject_context(
            current_query=current_query.strip(),
            max_items=max_items,
            relevance_threshold=relevance_threshold,
            include_solutions=include_solutions,
            include_conversations=include_conversations
        )
        
        # æ£€æŸ¥æ³¨å…¥ç»“æœ
        if not result:
            raise RuntimeError("ä¸Šä¸‹æ–‡æ³¨å…¥å·¥å…·è¿”å›ç©ºç»“æœ")
        
        context_items_count = result.get("total_items", 0)
        processing_time = result.get("processing_time_ms", 0)
        
        if ctx:
            if context_items_count > 0:
                await ctx.info(f"æˆåŠŸæ³¨å…¥ {context_items_count} ä¸ªç›¸å…³ä¸Šä¸‹æ–‡é¡¹")
                
                # æä¾›è¯¦ç»†çš„æ³¨å…¥ç»Ÿè®¡ä¿¡æ¯
                stats = result.get("search_statistics", {})
                if stats:
                    candidates = stats.get("candidates_found", 0)
                    above_threshold = stats.get("above_threshold", 0)
                    if candidates > 0:
                        await ctx.info(f"æ³¨å…¥ç»Ÿè®¡: {candidates} ä¸ªå€™é€‰ â†’ {above_threshold} ä¸ªç¬¦åˆé˜ˆå€¼ â†’ {context_items_count} ä¸ªæœ€ç»ˆé€‰æ‹©")
                
                # æ€§èƒ½ä¿¡æ¯
                if processing_time > 300:
                    await ctx.info(f"å¤„ç†æ—¶é—´: {processing_time:.2f}ms (ç›®æ ‡<500ms)")
                else:
                    await ctx.info(f"å¤„ç†é«˜æ•ˆ: {processing_time:.2f}ms")
            else:
                await ctx.info(f"æœªæ‰¾åˆ°ç¬¦åˆé˜ˆå€¼ {relevance_threshold} çš„ç›¸å…³ä¸Šä¸‹æ–‡")
        
        # ç¡®ä¿è¿”å›æ ¼å¼ä¸åŸAPIå…¼å®¹ï¼ŒåŒæ—¶æä¾›æ‰©å±•ä¿¡æ¯
        return {
            # åŸºæœ¬å…¼å®¹ä¿¡æ¯
            "context_items": result.get("context_items", []),
            "injection_summary": result.get("injection_summary", "ä¸Šä¸‹æ–‡æ³¨å…¥å®Œæˆ"),
            "total_items": context_items_count,
            "query": current_query,
            "relevance_threshold": relevance_threshold,
            
            # æ‰©å±•ä¿¡æ¯
            "processed_query": result.get("processed_query", current_query),
            "keywords_extracted": result.get("keywords_extracted", []),
            "processing_time_ms": round(processing_time, 2),
            "search_statistics": result.get("search_statistics", {}),
            "settings": {
                "max_items": max_items,
                "include_solutions": include_solutions,
                "include_conversations": include_conversations
            },
            "success": True,
            "context_engine": "synapse_intelligent_inject_v1.0"
        }
        
    except Exception as e:
        error_msg = str(e)
        if ctx:
            await ctx.error(f"ä¸Šä¸‹æ–‡æ³¨å…¥å¤±è´¥: {error_msg}")
        
        logger.error(f"ä¸Šä¸‹æ–‡æ³¨å…¥å¤±è´¥ - æŸ¥è¯¢: '{current_query}', é”™è¯¯: {error_msg}", exc_info=True)
        
        # è¿”å›é”™è¯¯ä¿¡æ¯è€Œä¸æ˜¯æŠ›å‡ºå¼‚å¸¸ï¼Œè®©è°ƒç”¨æ–¹èƒ½å¤Ÿå¤„ç†
        return {
            "context_items": [],
            "injection_summary": f"æ³¨å…¥å¤±è´¥: {error_msg}",
            "total_items": 0,
            "query": current_query,
            "error": error_msg,
            "error_type": type(e).__name__,
            "relevance_threshold": relevance_threshold,
            "settings": {
                "max_items": max_items,
                "include_solutions": include_solutions,
                "include_conversations": include_conversations
            },
            "success": False,
            "context_engine": "synapse_intelligent_inject_v1.0"
        }

@mcp.tool()
async def export_data(
    export_path: str,
    include_backups: bool = False,
    include_cache: bool = False,
    ctx: Context = None
) -> dict:
    """
    Export all Synapse data to a specified directory path.
    
    This tool exports conversations, solutions, indexes, and optionally backups
    to enable data migration, backup, and sharing between systems.
    
    Args:
        export_path: Target directory path for exported data (required)
        include_backups: Whether to include backup files in export (default: False)
        include_cache: Whether to include cache files in export (default: False)
        ctx: MCP context object for logging and progress reporting
        
    Returns:
        dict: Export result with status, statistics, and file paths
    """
    try:
        if ctx:
            await ctx.info(f"Starting data export to: {export_path}")
        
        # Parameter validation
        if not export_path or not export_path.strip():
            raise ValueError("Export path cannot be empty")
        
        from pathlib import Path
        export_dir = Path(export_path.strip()).expanduser().resolve()
        
        # Check if export path is valid and writable
        if export_dir.exists() and not export_dir.is_dir():
            raise ValueError(f"Export path exists but is not a directory: {export_dir}")
        
        # Get file manager instance
        file_manager = None
        if ctx and hasattr(ctx, 'request_context') and hasattr(ctx.request_context, 'lifespan_context'):
            file_manager = ctx.request_context.lifespan_context.file_manager
        
        if not file_manager:
            raise RuntimeError("æ— æ³•è·å–FileManagerå®ä¾‹ï¼Œè¯·æ£€æŸ¥æœåŠ¡å™¨é…ç½®")
        
        if ctx:
            await ctx.info("Validating export directory permissions...")
        
        # Test write permissions by creating export directory
        try:
            export_dir.mkdir(parents=True, exist_ok=True)
        except PermissionError:
            raise PermissionError(f"No write permission for export path: {export_dir}")
        except Exception as e:
            raise RuntimeError(f"Failed to create export directory: {e}")
        
        if ctx:
            await ctx.info("Gathering storage statistics...")
        
        # Get pre-export statistics
        storage_stats = file_manager.get_storage_statistics()
        
        if ctx:
            await ctx.info(f"Found {storage_stats.total_conversations} conversations and {storage_stats.total_solutions} solutions")
            await ctx.report_progress(progress=0.2, message="Starting data export...")
        
        # Perform the export using FileManager
        export_success = file_manager.export_data(export_dir, include_backups=include_backups)
        
        if not export_success:
            raise RuntimeError("Export operation failed - check file manager logs for details")
        
        if ctx:
            await ctx.report_progress(progress=0.8, message="Finalizing export...")
        
        if ctx:
            await ctx.report_progress(progress=1.0, message="Export completed successfully")
            await ctx.info(f"Exported {storage_stats.total_files} files ({storage_stats.disk_usage_mb:.2f} MB) to {export_dir}")
        
        # Build comprehensive response
        result = {
            "success": True,
            "export_path": str(export_dir),
            "exported_at": datetime.now().isoformat(),
            "statistics": {
                "total_conversations_exported": storage_stats.total_conversations,
                "total_solutions_exported": storage_stats.total_solutions,
                "total_files_exported": storage_stats.total_files,
                "total_size_bytes": storage_stats.total_size_bytes,
                "total_size_mb": round(storage_stats.disk_usage_mb, 2),
                "include_backups": include_backups,
                "include_cache": include_cache
            },
            "exported_components": {
                "conversations": True,
                "solutions": True,
                "indexes": True,
                "backups": include_backups,
                "cache": include_cache,
                "export_info": True
            },
            "summary": f"Successfully exported {storage_stats.total_conversations} conversations and {storage_stats.total_solutions} solutions to {export_dir}"
        }
        
        return result
        
    except Exception as e:
        error_msg = str(e)
        if ctx:
            await ctx.error(f"Data export failed: {error_msg}")
        
        logger.error(f"æ•°æ®å¯¼å‡ºå¤±è´¥ - è·¯å¾„: {export_path}, é”™è¯¯: {error_msg}", exc_info=True)
        
        return {
            "success": False,
            "export_path": export_path,
            "error": error_msg,
            "error_type": type(e).__name__,
            "exported_at": datetime.now().isoformat(),
            "statistics": {
                "total_conversations_exported": 0,
                "total_solutions_exported": 0,
                "total_files_exported": 0,
                "total_size_bytes": 0,
                "total_size_mb": 0,
                "include_backups": include_backups,
                "include_cache": include_cache
            },
            "summary": f"Export failed: {error_msg}"
        }

@mcp.tool()
async def import_data(
    import_path: str,
    merge_mode: str = "append",
    validate_data: bool = True,
    create_backup: bool = True,
    ctx: Context = None
) -> dict:
    """
    Import Synapse data from a specified directory path.
    
    This tool imports conversations, solutions, and indexes from an exported
    data directory, with options for data validation and merge strategies.
    
    Args:
        import_path: Source directory path containing exported data (required)
        merge_mode: Merge strategy - "append" or "overwrite" (default: "append")
        validate_data: Whether to validate data format before import (default: True)
        create_backup: Whether to create backup before import (default: True)
        ctx: MCP context object for logging and progress reporting
        
    Returns:
        dict: Import result with status, statistics, and validation results
    """
    try:
        if ctx:
            await ctx.info(f"Starting data import from: {import_path}")
        
        # Parameter validation
        if not import_path or not import_path.strip():
            raise ValueError("Import path cannot be empty")
        
        if merge_mode not in ["append", "overwrite"]:
            raise ValueError("Merge mode must be 'append' or 'overwrite'")
        
        from pathlib import Path
        import_dir = Path(import_path.strip()).expanduser().resolve()
        
        # Check if import path exists and is valid
        if not import_dir.exists():
            raise FileNotFoundError(f"Import path does not exist: {import_dir}")
        
        if not import_dir.is_dir():
            raise ValueError(f"Import path is not a directory: {import_dir}")
        
        # Get file manager instance
        file_manager = None
        if ctx and hasattr(ctx, 'request_context') and hasattr(ctx.request_context, 'lifespan_context'):
            file_manager = ctx.request_context.lifespan_context.file_manager
        
        if not file_manager:
            raise RuntimeError("æ— æ³•è·å–FileManagerå®ä¾‹ï¼Œè¯·æ£€æŸ¥æœåŠ¡å™¨é…ç½®")
        
        if ctx:
            await ctx.info("Validating import data structure...")
            await ctx.report_progress(progress=0.1, message="Validating import data...")
        
        # Validate import structure
        validation_results = {
            "conversations_found": False,
            "solutions_found": False,
            "indexes_found": False,
            "export_info_found": False,
            "data_valid": True,
            "validation_errors": []
        }
        
        # Check for expected directories and files
        conversations_dir = import_dir / "conversations"
        solutions_dir = import_dir / "solutions"
        indexes_dir = import_dir / "indexes"
        export_info_file = import_dir / "export_info.json"
        
        validation_results["conversations_found"] = conversations_dir.exists() and conversations_dir.is_dir()
        validation_results["solutions_found"] = solutions_dir.exists() and solutions_dir.is_dir()
        validation_results["indexes_found"] = indexes_dir.exists() and indexes_dir.is_dir()
        validation_results["export_info_found"] = export_info_file.exists() and export_info_file.is_file()
        
        # Basic structure validation
        if not any([validation_results["conversations_found"], validation_results["solutions_found"], validation_results["indexes_found"]]):
            validation_results["data_valid"] = False
            validation_results["validation_errors"].append("No valid data directories found (conversations, solutions, or indexes)")
        
        # Data format validation if requested
        if validate_data and validation_results["data_valid"]:
            if ctx:
                await ctx.info("Performing detailed data validation...")
            
            # Count and validate files
            file_counts = {"conversations": 0, "solutions": 0, "invalid_files": 0}
            
            try:
                # Check conversation files
                if conversations_dir.exists():
                    for conv_file in conversations_dir.rglob("*.json"):
                        if conv_file.is_file():
                            try:
                                with open(conv_file, 'r', encoding='utf-8') as f:
                                    conv_data = json.load(f)
                                    # Basic structure validation
                                    if not all(key in conv_data for key in ["id", "title", "content"]):
                                        file_counts["invalid_files"] += 1
                                        validation_results["validation_errors"].append(f"Invalid conversation format: {conv_file.name}")
                                    else:
                                        file_counts["conversations"] += 1
                            except Exception as e:
                                file_counts["invalid_files"] += 1
                                validation_results["validation_errors"].append(f"Cannot read conversation file {conv_file.name}: {str(e)}")
                
                # Check solution files
                if solutions_dir.exists():
                    for sol_file in solutions_dir.glob("*.json"):
                        if sol_file.is_file():
                            try:
                                with open(sol_file, 'r', encoding='utf-8') as f:
                                    sol_data = json.load(f)
                                    # Basic structure validation
                                    if not all(key in sol_data for key in ["id", "type", "content"]):
                                        file_counts["invalid_files"] += 1
                                        validation_results["validation_errors"].append(f"Invalid solution format: {sol_file.name}")
                                    else:
                                        file_counts["solutions"] += 1
                            except Exception as e:
                                file_counts["invalid_files"] += 1
                                validation_results["validation_errors"].append(f"Cannot read solution file {sol_file.name}: {str(e)}")
                
                if file_counts["invalid_files"] > 0:
                    validation_results["data_valid"] = False
                    
            except Exception as e:
                validation_results["data_valid"] = False
                validation_results["validation_errors"].append(f"Data validation failed: {str(e)}")
        
        if ctx:
            if validation_results["data_valid"]:
                await ctx.info(f"Validation passed - Found {file_counts.get('conversations', 0)} conversations and {file_counts.get('solutions', 0)} solutions")
            else:
                await ctx.info(f"Validation found {len(validation_results['validation_errors'])} issues")
        
        # Proceed with import if validation passes or validation is disabled
        if not validate_data or validation_results["data_valid"]:
            
            # Create backup if requested
            backup_info = None
            if create_backup:
                if ctx:
                    await ctx.info("Creating pre-import backup...")
                    await ctx.report_progress(progress=0.3, message="Creating backup...")
                
                try:
                    backup_timestamp = int(datetime.now().timestamp())
                    backup_path = file_manager.storage_paths.get_backups_dir() / f"pre_import_backup_{backup_timestamp}"
                    backup_success = file_manager.export_data(backup_path, include_backups=False)
                    
                    if backup_success:
                        backup_info = str(backup_path)
                        if ctx:
                            await ctx.info(f"Backup created successfully: {backup_path}")
                    else:
                        logger.warning("Pre-import backup failed, continuing with import")
                        
                except Exception as e:
                    logger.warning(f"Pre-import backup failed: {e}")
            
            if ctx:
                await ctx.info(f"Starting import with merge mode: {merge_mode}")
                await ctx.report_progress(progress=0.5, message="Importing data...")
            
            # Perform the import using FileManager
            import_success = file_manager.import_data(import_dir, merge_mode=merge_mode)
            
            if not import_success:
                raise RuntimeError("Import operation failed - check file manager logs for details")
            
            if ctx:
                await ctx.report_progress(progress=0.9, message="Finalizing import...")
            
            # Get post-import statistics
            post_import_stats = file_manager.get_storage_statistics()
            
            if ctx:
                await ctx.report_progress(progress=1.0, message="Import completed successfully")
                await ctx.info(f"Import completed - Total: {post_import_stats.total_conversations} conversations, {post_import_stats.total_solutions} solutions")
            
            # Build success response
            result = {
                "success": True,
                "import_path": str(import_dir),
                "imported_at": datetime.now().isoformat(),
                "merge_mode": merge_mode,
                "validation_performed": validate_data,
                "backup_created": create_backup,
                "backup_location": backup_info,
                "validation_results": validation_results,
                "statistics": {
                    "conversations_after_import": post_import_stats.total_conversations,
                    "solutions_after_import": post_import_stats.total_solutions,
                    "total_files_after_import": post_import_stats.total_files,
                    "total_size_mb_after_import": round(post_import_stats.disk_usage_mb, 2)
                },
                "imported_components": {
                    "conversations": validation_results["conversations_found"],
                    "solutions": validation_results["solutions_found"],
                    "indexes": validation_results["indexes_found"]
                },
                "summary": f"Successfully imported data from {import_dir} using {merge_mode} mode"
            }
            
            return result
        
        else:
            # Validation failed
            raise ValueError(f"Data validation failed: {', '.join(validation_results['validation_errors'])}")
        
    except Exception as e:
        error_msg = str(e)
        if ctx:
            await ctx.error(f"Data import failed: {error_msg}")
        
        logger.error(f"æ•°æ®å¯¼å…¥å¤±è´¥ - è·¯å¾„: {import_path}, é”™è¯¯: {error_msg}", exc_info=True)
        
        return {
            "success": False,
            "import_path": import_path,
            "error": error_msg,
            "error_type": type(e).__name__,
            "imported_at": datetime.now().isoformat(),
            "merge_mode": merge_mode,
            "validation_performed": validate_data,
            "backup_created": create_backup,
            "validation_results": validation_results if 'validation_results' in locals() else {},
            "summary": f"Import failed: {error_msg}"
        }

@mcp.tool()
async def get_storage_info(ctx: Context = None) -> dict:
    """
    Get comprehensive storage system information and statistics.
    
    This tool provides detailed information about Synapse storage locations,
    directory status, usage statistics, and system health.
    
    Args:
        ctx: MCP context object for logging and progress reporting
        
    Returns:
        dict: Complete storage system information including:
            - Storage directory paths
            - Directory existence and permissions status
            - Storage usage statistics
            - Initialization status
            - System health information
    """
    try:
        if ctx:
            await ctx.info("Retrieving storage system information")
        
        # Initialize storage system components
        storage_paths = StoragePaths()
        initializer = StorageInitializer(storage_paths)
        
        # Get file manager instance for statistics
        file_manager = None
        if ctx and hasattr(ctx, 'request_context') and hasattr(ctx.request_context, 'lifespan_context'):
            file_manager = ctx.request_context.lifespan_context.file_manager
        
        # Get comprehensive storage status
        storage_status = initializer.get_storage_status()
        
        # Calculate storage size in MB
        total_size_mb = storage_status["total_size_bytes"] / (1024 * 1024) if storage_status["total_size_bytes"] > 0 else 0.0
        
        # Get real storage statistics if file manager is available
        storage_stats = None
        conversation_count = 0
        solution_count = 0
        
        if file_manager:
            if ctx:
                await ctx.info("Gathering detailed storage statistics...")
            try:
                storage_stats = file_manager.get_storage_statistics()
                conversation_count = storage_stats.total_conversations
                solution_count = storage_stats.total_solutions
            except Exception as e:
                logger.warning(f"Failed to get detailed storage statistics: {e}")
        
        # Get backup information
        backup_info = {
            "backups_available": 0,
            "latest_backup": None,
            "total_backup_size_mb": 0.0,
            "backup_directory_status": "unknown"
        }
        
        try:
            backups_dir = storage_paths.get_backups_dir()
            if backups_dir.exists():
                backup_files = list(backups_dir.glob("*backup*"))
                backup_info["backups_available"] = len([f for f in backup_files if f.is_dir()])
                backup_info["backup_directory_status"] = "available"
                
                # Find latest backup
                if backup_files:
                    latest_backup = max(backup_files, key=lambda x: x.stat().st_mtime if x.exists() else 0)
                    backup_info["latest_backup"] = {
                        "path": str(latest_backup),
                        "created_at": datetime.fromtimestamp(latest_backup.stat().st_mtime).isoformat(),
                        "size_mb": round(sum(f.stat().st_size for f in latest_backup.rglob("*") if f.is_file()) / (1024*1024), 2)
                    }
                
                # Calculate total backup size
                try:
                    total_backup_size = sum(
                        sum(f.stat().st_size for f in backup_dir.rglob("*") if f.is_file())
                        for backup_dir in backup_files if backup_dir.is_dir()
                    )
                    backup_info["total_backup_size_mb"] = round(total_backup_size / (1024*1024), 2)
                except Exception:
                    backup_info["total_backup_size_mb"] = 0.0
            else:
                backup_info["backup_directory_status"] = "not_created"
        except Exception as e:
            logger.warning(f"Failed to get backup information: {e}")
            backup_info["backup_directory_status"] = "error"
        
        # System health check
        health_status = {
            "overall_status": "healthy",
            "issues": [],
            "warnings": []
        }
        
        # Check for potential issues
        if not storage_status["permissions_ok"]:
            health_status["issues"].append("Insufficient permissions for some storage directories")
            health_status["overall_status"] = "degraded"
        
        if total_size_mb > 1000:  # > 1GB
            health_status["warnings"].append(f"Storage usage is high: {total_size_mb:.1f} MB")
        
        if storage_stats and storage_stats.total_files > 5000:
            health_status["warnings"].append(f"Large number of files: {storage_stats.total_files}")
        
        if backup_info["backups_available"] == 0:
            health_status["warnings"].append("No backups available")
        
        # Maintenance information
        maintenance_info = {
            "last_maintenance": None,
            "maintenance_needed": False,
            "recommended_actions": []
        }
        
        # Check if maintenance is needed
        if backup_info["backups_available"] > 10:
            maintenance_info["maintenance_needed"] = True
            maintenance_info["recommended_actions"].append("Clean up old backup files")
        
        if storage_stats and storage_stats.disk_usage_mb > 500:
            maintenance_info["maintenance_needed"] = True
            maintenance_info["recommended_actions"].append("Consider archiving old conversations")
        
        # Try to get last maintenance timestamp from metadata
        try:
            metadata_file = storage_paths.get_indexes_dir() / "metadata.json"
            if metadata_file.exists():
                with open(metadata_file, 'r', encoding='utf-8') as f:
                    metadata = json.load(f)
                    maintenance_info["last_maintenance"] = metadata.get("last_maintenance")
        except Exception:
            pass
        
        # Build comprehensive response with enhanced information
        storage_info = {
            # Primary storage locations
            "data_directory": storage_status["storage_paths"]["data"],
            "config_directory": storage_status["storage_paths"]["config"],
            "cache_directory": storage_status["storage_paths"]["cache"],
            
            # Data subdirectories
            "conversations_directory": storage_status["storage_paths"]["conversations"],
            "solutions_directory": storage_status["storage_paths"]["solutions"],
            "indexes_directory": storage_status["storage_paths"]["indexes"],
            "logs_directory": storage_status["storage_paths"]["logs"],
            "backups_directory": storage_status["storage_paths"]["backups"],
            
            # System status
            "initialized": storage_status["initialized"],
            "permissions_ok": storage_status["permissions_ok"],
            "total_storage_size_bytes": storage_status["total_size_bytes"],
            "total_storage_size_mb": round(total_size_mb, 2),
            
            # Directory details
            "directory_status": storage_status["directory_status"],
            
            # Real statistics
            "total_conversations": conversation_count,
            "total_solutions": solution_count,
            "total_files": storage_stats.total_files if storage_stats else 0,
            "average_file_size_kb": round(storage_stats.avg_file_size_kb, 2) if storage_stats else 0,
            
            # Backup information
            "backup_info": backup_info,
            
            # System health
            "health_status": health_status,
            
            # Maintenance information
            "maintenance_info": maintenance_info,
            
            # Application info
            "server_status": "running",
            "version": "1.0.0",
            "platform": sys.platform,
            "last_updated": datetime.now().isoformat()
        }
        
        if ctx:
            await ctx.info(f"Storage information retrieved successfully - Initialized: {storage_status['initialized']}")
        
        return storage_info
        
    except Exception as e:
        if ctx:
            await ctx.error(f"Failed to retrieve storage information: {str(e)}")
        logger.error(f"Failed to retrieve storage information: {str(e)}", exc_info=True)
        raise ValueError(f"Failed to retrieve storage information: {str(e)}")

@mcp.tool()
async def backup_data(
    backup_name: str = None,
    include_cache: bool = False,
    compression: bool = False,
    ctx: Context = None
) -> dict:
    """
    Create a manual backup of all Synapse data.
    
    This tool creates a comprehensive backup of conversations, solutions, indexes,
    and configuration files for disaster recovery purposes.
    
    Args:
        backup_name: Custom name for the backup (optional, auto-generated if not provided)
        include_cache: Whether to include cache files in backup (default: False)
        compression: Whether to compress the backup (default: False - not implemented)
        ctx: MCP context object for logging and progress reporting
        
    Returns:
        dict: Backup result with status, location, and statistics
    """
    try:
        if ctx:
            await ctx.info("Starting manual data backup...")
        
        # Get file manager instance
        file_manager = None
        if ctx and hasattr(ctx, 'request_context') and hasattr(ctx.request_context, 'lifespan_context'):
            file_manager = ctx.request_context.lifespan_context.file_manager
        
        if not file_manager:
            raise RuntimeError("æ— æ³•è·å–FileManagerå®ä¾‹ï¼Œè¯·æ£€æŸ¥æœåŠ¡å™¨é…ç½®")
        
        # Generate backup name if not provided
        if not backup_name:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            backup_name = f"manual_backup_{timestamp}"
        else:
            # Sanitize backup name
            import re
            backup_name = re.sub(r'[^\w\-_.]', '_', backup_name.strip())
            if not backup_name:
                backup_name = f"manual_backup_{int(datetime.now().timestamp())}"
        
        if ctx:
            await ctx.info(f"Creating backup: {backup_name}")
            await ctx.report_progress(progress=0.1, message="Preparing backup...")
        
        # Get pre-backup statistics
        storage_stats = file_manager.get_storage_statistics()
        
        # Create backup directory
        backup_path = file_manager.storage_paths.get_backups_dir() / backup_name
        
        if backup_path.exists():
            raise ValueError(f"Backup already exists: {backup_name}")
        
        if ctx:
            await ctx.info(f"Backup location: {backup_path}")
            await ctx.report_progress(progress=0.3, message="Creating backup directory...")
        
        # Perform the backup
        backup_success = file_manager.export_data(backup_path, include_backups=False)
        
        if not backup_success:
            raise RuntimeError("Backup operation failed - check file manager logs for details")
        
        if ctx:
            await ctx.report_progress(progress=0.8, message="Calculating backup statistics...")
        
        # Calculate backup statistics
        backup_stats = {
            "backup_size_bytes": 0,
            "backup_size_mb": 0.0,
            "files_backed_up": 0,
            "directories_backed_up": 0
        }
        
        try:
            if backup_path.exists():
                for item in backup_path.rglob("*"):
                    if item.is_file():
                        backup_stats["files_backed_up"] += 1
                        backup_stats["backup_size_bytes"] += item.stat().st_size
                    elif item.is_dir():
                        backup_stats["directories_backed_up"] += 1
                
                backup_stats["backup_size_mb"] = round(backup_stats["backup_size_bytes"] / (1024*1024), 2)
        except Exception as e:
            logger.warning(f"Failed to calculate backup statistics: {e}")
        
        # Create backup metadata
        backup_metadata = {
            "backup_name": backup_name,
            "created_at": datetime.now().isoformat(),
            "backup_type": "manual",
            "source_statistics": {
                "conversations_count": storage_stats.total_conversations,
                "solutions_count": storage_stats.total_solutions,
                "total_files": storage_stats.total_files,
                "source_size_mb": round(storage_stats.disk_usage_mb, 2)
            },
            "backup_statistics": backup_stats,
            "include_cache": include_cache,
            "compression": compression,
            "version": "1.0.0"
        }
        
        # Save backup metadata
        try:
            metadata_file = backup_path / "backup_metadata.json"
            with open(metadata_file, 'w', encoding='utf-8') as f:
                json.dump(backup_metadata, f, indent=2, ensure_ascii=False)
        except Exception as e:
            logger.warning(f"Failed to save backup metadata: {e}")
        
        if ctx:
            await ctx.report_progress(progress=1.0, message="Backup completed successfully")
            await ctx.info(f"Backup created successfully: {backup_stats['files_backed_up']} files ({backup_stats['backup_size_mb']} MB)")
        
        result = {
            "success": True,
            "backup_name": backup_name,
            "backup_path": str(backup_path),
            "created_at": datetime.now().isoformat(),
            "backup_type": "manual",
            "statistics": backup_stats,
            "source_data": {
                "conversations_backed_up": storage_stats.total_conversations,
                "solutions_backed_up": storage_stats.total_solutions,
                "total_source_files": storage_stats.total_files
            },
            "options": {
                "include_cache": include_cache,
                "compression": compression
            },
            "summary": f"Successfully created backup '{backup_name}' with {backup_stats['files_backed_up']} files ({backup_stats['backup_size_mb']} MB)"
        }
        
        return result
        
    except Exception as e:
        error_msg = str(e)
        if ctx:
            await ctx.error(f"Backup creation failed: {error_msg}")
        
        logger.error(f"å¤‡ä»½åˆ›å»ºå¤±è´¥ - åç§°: {backup_name}, é”™è¯¯: {error_msg}", exc_info=True)
        
        return {
            "success": False,
            "backup_name": backup_name or "unknown",
            "error": error_msg,
            "error_type": type(e).__name__,
            "created_at": datetime.now().isoformat(),
            "summary": f"Backup creation failed: {error_msg}"
        }

@mcp.tool()
async def restore_backup(
    backup_name: str,
    restore_mode: str = "append",
    verify_backup: bool = True,
    create_restore_backup: bool = True,
    ctx: Context = None
) -> dict:
    """
    Restore Synapse data from a previously created backup.
    
    This tool restores conversations, solutions, and indexes from a backup
    directory with options for verification and pre-restore backup.
    
    Args:
        backup_name: Name of the backup to restore (required)
        restore_mode: Restore strategy - "append" or "overwrite" (default: "append")
        verify_backup: Whether to verify backup integrity before restore (default: True)
        create_restore_backup: Whether to backup current data before restore (default: True)
        ctx: MCP context object for logging and progress reporting
        
    Returns:
        dict: Restore result with status, statistics, and verification results
    """
    try:
        if ctx:
            await ctx.info(f"Starting backup restore: {backup_name}")
        
        # Parameter validation
        if not backup_name or not backup_name.strip():
            raise ValueError("Backup name cannot be empty")
        
        if restore_mode not in ["append", "overwrite"]:
            raise ValueError("Restore mode must be 'append' or 'overwrite'")
        
        # Get file manager instance
        file_manager = None
        if ctx and hasattr(ctx, 'request_context') and hasattr(ctx.request_context, 'lifespan_context'):
            file_manager = ctx.request_context.lifespan_context.file_manager
        
        if not file_manager:
            raise RuntimeError("æ— æ³•è·å–FileManagerå®ä¾‹ï¼Œè¯·æ£€æŸ¥æœåŠ¡å™¨é…ç½®")
        
        # Locate backup directory
        backup_path = file_manager.storage_paths.get_backups_dir() / backup_name.strip()
        
        if not backup_path.exists():
            raise FileNotFoundError(f"Backup not found: {backup_name}")
        
        if not backup_path.is_dir():
            raise ValueError(f"Backup path is not a directory: {backup_name}")
        
        if ctx:
            await ctx.info(f"Found backup at: {backup_path}")
            await ctx.report_progress(progress=0.1, message="Verifying backup...")
        
        # Backup verification
        verification_results = {
            "backup_valid": True,
            "verification_errors": [],
            "backup_metadata": None,
            "conversations_found": 0,
            "solutions_found": 0
        }
        
        if verify_backup:
            try:
                # Check backup structure
                conversations_dir = backup_path / "conversations"
                solutions_dir = backup_path / "solutions"
                indexes_dir = backup_path / "indexes"
                metadata_file = backup_path / "backup_metadata.json"
                
                if not any([conversations_dir.exists(), solutions_dir.exists(), indexes_dir.exists()]):
                    verification_results["backup_valid"] = False
                    verification_results["verification_errors"].append("No valid data directories found in backup")
                
                # Load backup metadata if available
                if metadata_file.exists():
                    try:
                        with open(metadata_file, 'r', encoding='utf-8') as f:
                            verification_results["backup_metadata"] = json.load(f)
                    except Exception as e:
                        verification_results["verification_errors"].append(f"Cannot read backup metadata: {str(e)}")
                
                # Count backup contents
                if conversations_dir.exists():
                    verification_results["conversations_found"] = len(list(conversations_dir.rglob("*.json")))
                
                if solutions_dir.exists():
                    verification_results["solutions_found"] = len(list(solutions_dir.glob("*.json")))
                
                if verification_results["conversations_found"] == 0 and verification_results["solutions_found"] == 0:
                    verification_results["backup_valid"] = False
                    verification_results["verification_errors"].append("No conversation or solution files found in backup")
                
            except Exception as e:
                verification_results["backup_valid"] = False
                verification_results["verification_errors"].append(f"Backup verification failed: {str(e)}")
        
        if verify_backup and not verification_results["backup_valid"]:
            raise ValueError(f"Backup verification failed: {', '.join(verification_results['verification_errors'])}")
        
        if ctx:
            if verification_results["backup_valid"]:
                await ctx.info(f"Backup verified - {verification_results['conversations_found']} conversations, {verification_results['solutions_found']} solutions")
            await ctx.report_progress(progress=0.3, message="Creating pre-restore backup...")
        
        # Create pre-restore backup if requested
        restore_backup_info = None
        if create_restore_backup:
            try:
                restore_backup_timestamp = int(datetime.now().timestamp())
                restore_backup_path = file_manager.storage_paths.get_backups_dir() / f"pre_restore_backup_{restore_backup_timestamp}"
                restore_backup_success = file_manager.export_data(restore_backup_path, include_backups=False)
                
                if restore_backup_success:
                    restore_backup_info = str(restore_backup_path)
                    if ctx:
                        await ctx.info(f"Pre-restore backup created: {restore_backup_path}")
                else:
                    logger.warning("Pre-restore backup failed, continuing with restore")
                    
            except Exception as e:
                logger.warning(f"Pre-restore backup failed: {e}")
        
        if ctx:
            await ctx.info(f"Starting restore with mode: {restore_mode}")
            await ctx.report_progress(progress=0.5, message="Restoring data...")
        
        # Perform the restore using FileManager import
        restore_success = file_manager.import_data(backup_path, merge_mode=restore_mode)
        
        if not restore_success:
            raise RuntimeError("Restore operation failed - check file manager logs for details")
        
        if ctx:
            await ctx.report_progress(progress=0.9, message="Finalizing restore...")
        
        # Get post-restore statistics
        post_restore_stats = file_manager.get_storage_statistics()
        
        if ctx:
            await ctx.report_progress(progress=1.0, message="Restore completed successfully")
            await ctx.info(f"Restore completed - Total: {post_restore_stats.total_conversations} conversations, {post_restore_stats.total_solutions} solutions")
        
        # Build success response
        result = {
            "success": True,
            "backup_name": backup_name,
            "backup_path": str(backup_path),
            "restored_at": datetime.now().isoformat(),
            "restore_mode": restore_mode,
            "verification_performed": verify_backup,
            "pre_restore_backup_created": create_restore_backup,
            "pre_restore_backup_location": restore_backup_info,
            "verification_results": verification_results,
            "statistics": {
                "conversations_after_restore": post_restore_stats.total_conversations,
                "solutions_after_restore": post_restore_stats.total_solutions,
                "total_files_after_restore": post_restore_stats.total_files,
                "total_size_mb_after_restore": round(post_restore_stats.disk_usage_mb, 2)
            },
            "restored_components": {
                "conversations": verification_results["conversations_found"] > 0,
                "solutions": verification_results["solutions_found"] > 0,
                "indexes": True  # Assume indexes are always restored if present
            },
            "summary": f"Successfully restored backup '{backup_name}' using {restore_mode} mode"
        }
        
        return result
        
    except Exception as e:
        error_msg = str(e)
        if ctx:
            await ctx.error(f"Backup restore failed: {error_msg}")
        
        logger.error(f"å¤‡ä»½æ¢å¤å¤±è´¥ - å¤‡ä»½: {backup_name}, é”™è¯¯: {error_msg}", exc_info=True)
        
        return {
            "success": False,
            "backup_name": backup_name,
            "error": error_msg,
            "error_type": type(e).__name__,
            "restored_at": datetime.now().isoformat(),
            "restore_mode": restore_mode,
            "verification_performed": verify_backup,
            "pre_restore_backup_created": create_restore_backup,
            "verification_results": verification_results if 'verification_results' in locals() else {},
            "summary": f"Restore failed: {error_msg}"
        }

@mcp.tool()
async def extract_solutions(
    conversation_id: str = None,
    extract_type: str = "all",
    min_reusability_score: float = 0.3,
    save_solutions: bool = True,
    overwrite_existing: bool = False,
    ctx: Context = None
) -> dict:
    """
    ä»å·²ä¿å­˜çš„å¯¹è¯è®°å½•ä¸­æå–è§£å†³æ–¹æ¡ˆå¹¶å•ç‹¬ä¿å­˜
    
    è¿™ä¸ªå·¥å…·ä»ç°æœ‰çš„å¯¹è¯è®°å½•ä¸­æå–æ‰€æœ‰è§£å†³æ–¹æ¡ˆï¼Œè¿›è¡Œè´¨é‡è¯„ä¼°å’Œå»é‡ï¼Œ
    ç„¶åå°†é«˜è´¨é‡çš„è§£å†³æ–¹æ¡ˆä¿å­˜åˆ°ç‹¬ç«‹çš„solutionsç›®å½•ä¸­ï¼Œä¾¿äºåç»­æŸ¥æ‰¾å’Œé‡ç”¨ã€‚
    
    ä½¿ç”¨åœºæ™¯ï¼š
    1. æ‰¹é‡æå–æ‰€æœ‰å¯¹è¯ä¸­çš„è§£å†³æ–¹æ¡ˆ
    2. ä»ç‰¹å®šå¯¹è¯ä¸­æå–è§£å†³æ–¹æ¡ˆ
    3. æŒ‰ç±»å‹ç­›é€‰è§£å†³æ–¹æ¡ˆï¼ˆä»£ç ã€æ–¹æ³•ã€æ¨¡å¼ï¼‰
    4. è´¨é‡ç­›é€‰å’Œå»é‡å¤„ç†
    
    Args:
        conversation_id: æŒ‡å®šå¯¹è¯IDï¼Œä¸ºNoneæ—¶å¤„ç†æ‰€æœ‰å¯¹è¯è®°å½•ï¼ˆå¯é€‰ï¼‰
        extract_type: æå–ç±»å‹ç­›é€‰ ("code", "approach", "pattern", "all")
        min_reusability_score: æœ€å°å¯é‡ç”¨æ€§åˆ†æ•°é˜ˆå€¼ (0.0-1.0)
        save_solutions: æ˜¯å¦å°†è§£å†³æ–¹æ¡ˆä¿å­˜åˆ°æ–‡ä»¶ç³»ç»Ÿ (default: True)
        overwrite_existing: æ˜¯å¦è¦†ç›–å·²å­˜åœ¨çš„è§£å†³æ–¹æ¡ˆæ–‡ä»¶ (default: False)
        ctx: MCPä¸Šä¸‹æ–‡å¯¹è±¡
        
    Returns:
        dict: æå–ç»“æœï¼ŒåŒ…å«è§£å†³æ–¹æ¡ˆåˆ—è¡¨ã€ç»Ÿè®¡ä¿¡æ¯å’Œä¿å­˜çŠ¶æ€
        {
            "success": bool,
            "solutions": [...],
            "total_extracted": int,
            "conversations_processed": int,
            "extraction_summary": str,
            "statistics": {...},
            "storage_info": {...}
        }
    """
    try:
        start_time = time.time()
        
        if ctx:
            await ctx.info(f"å¼€å§‹è§£å†³æ–¹æ¡ˆæå–ä»»åŠ¡")
            if conversation_id:
                await ctx.info(f"ç›®æ ‡å¯¹è¯: {conversation_id}")
            else:
                await ctx.info("å¤„ç†æ‰€æœ‰å¯¹è¯è®°å½•")
        
        # å‚æ•°éªŒè¯
        if extract_type not in ["code", "approach", "pattern", "all"]:
            raise ValueError("extract_typeå¿…é¡»æ˜¯ 'code', 'approach', 'pattern' æˆ– 'all'")
        
        if not isinstance(min_reusability_score, (int, float)) or min_reusability_score < 0 or min_reusability_score > 1:
            raise ValueError("min_reusability_scoreå¿…é¡»åœ¨0.0-1.0ä¹‹é—´")
        
        # è·å–æå–è§£å†³æ–¹æ¡ˆå·¥å…·å®ä¾‹
        extract_tool = None
        if ctx and hasattr(ctx, 'request_context') and hasattr(ctx.request_context, 'lifespan_context'):
            extract_tool = ctx.request_context.lifespan_context.extract_solutions_tool
        
        if not extract_tool:
            raise RuntimeError("æ— æ³•è·å–ExtractSolutionsToolå®ä¾‹ï¼Œè¯·æ£€æŸ¥æœåŠ¡å™¨é…ç½®")
        
        if ctx:
            await ctx.info(f"ä½¿ç”¨å‚æ•°: ç±»å‹={extract_type}, æœ€ä½è´¨é‡={min_reusability_score}")
        
        # æ‰§è¡Œè§£å†³æ–¹æ¡ˆæå–
        result = await extract_tool.extract_solutions(
            conversation_id=conversation_id,
            extract_type=extract_type,
            min_reusability_score=min_reusability_score,
            save_solutions=save_solutions,
            overwrite_existing=overwrite_existing,
            ctx=ctx
        )
        
        # æ£€æŸ¥æå–ç»“æœ
        if not result.get("success", False):
            error_msg = result.get("error", "æœªçŸ¥é”™è¯¯")
            raise RuntimeError(f"è§£å†³æ–¹æ¡ˆæå–å¤±è´¥: {error_msg}")
        
        # è®¡ç®—å¤„ç†æ—¶é—´
        processing_time = (time.time() - start_time) * 1000
        result["statistics"]["processing_time_ms"] = round(processing_time, 2)
        
        total_solutions = result.get("total_extracted", 0)
        conversations_processed = result.get("conversations_processed", 0)
        
        if ctx:
            await ctx.info(f"æå–å®Œæˆ: {total_solutions} ä¸ªè§£å†³æ–¹æ¡ˆæ¥è‡ª {conversations_processed} ä¸ªå¯¹è¯")
            
            if save_solutions:
                files_created = len(result.get("storage_info", {}).get("files_created", []))
                if files_created > 0:
                    await ctx.info(f"å·²ä¿å­˜åˆ° {files_created} ä¸ªæ–‡ä»¶")
            
            # æ€§èƒ½ä¿¡æ¯
            if processing_time > 1000:  # > 1ç§’
                await ctx.info(f"å¤„ç†æ—¶é—´: {processing_time:.2f}ms")
            else:
                await ctx.info(f"å¤„ç†é«˜æ•ˆ: {processing_time:.2f}ms")
        
        # å¢å¼ºè¿”å›ç»“æœæ ¼å¼
        enhanced_result = {
            # åŸºæœ¬å…¼å®¹ä¿¡æ¯
            "success": True,
            "solutions": result.get("solutions", []),
            "total_extracted": total_solutions,
            "conversations_processed": conversations_processed,
            "conversations_with_solutions": result.get("conversations_with_solutions", 0),
            "extraction_summary": result.get("extraction_summary", ""),
            
            # å¤„ç†å‚æ•°ä¿¡æ¯
            "extraction_parameters": {
                "conversation_id": conversation_id,
                "extract_type": extract_type,
                "min_reusability_score": min_reusability_score,
                "save_solutions": save_solutions,
                "overwrite_existing": overwrite_existing
            },
            
            # è¯¦ç»†ç»Ÿè®¡ä¿¡æ¯
            "statistics": result.get("statistics", {}),
            
            # å­˜å‚¨ä¿¡æ¯
            "storage_info": result.get("storage_info", {}),
            
            # å…ƒæ•°æ®
            "processing_time_ms": round(processing_time, 2),
            "extracted_at": datetime.now().isoformat(),
            "extraction_engine": "synapse_solution_extractor_v1.0"
        }
        
        return enhanced_result
        
    except Exception as e:
        error_msg = str(e)
        if ctx:
            await ctx.error(f"è§£å†³æ–¹æ¡ˆæå–å¤±è´¥: {error_msg}")
        
        logger.error(f"è§£å†³æ–¹æ¡ˆæå–å¤±è´¥ - å¯¹è¯: {conversation_id}, é”™è¯¯: {error_msg}", exc_info=True)
        
        # è¿”å›é”™è¯¯ä¿¡æ¯è€Œä¸æ˜¯æŠ›å‡ºå¼‚å¸¸
        return {
            "success": False,
            "error": error_msg,
            "error_type": type(e).__name__,
            "solutions": [],
            "total_extracted": 0,
            "conversations_processed": 0,
            "extraction_summary": f"æå–å¤±è´¥: {error_msg}",
            "extraction_parameters": {
                "conversation_id": conversation_id,
                "extract_type": extract_type,
                "min_reusability_score": min_reusability_score,
                "save_solutions": save_solutions,
                "overwrite_existing": overwrite_existing
            },
            "processing_time_ms": 0,
            "extracted_at": datetime.now().isoformat(),
            "extraction_engine": "synapse_solution_extractor_v1.0"
        }

# ==================== ä¸»å…¥å£å‡½æ•° ====================

async def main():
    """
    Synapse MCP æœåŠ¡å™¨çš„ä¸»å…¥å£ç‚¹
    
    è¿™ä¸ªå‡½æ•°å¯åŠ¨MCPæœåŠ¡å™¨å¹¶å¤„ç†å®¢æˆ·ç«¯è¿æ¥ã€‚
    å®ƒè®¾ç½®æ—¥å¿—ã€åˆå§‹åŒ–å­˜å‚¨ç³»ç»Ÿï¼Œç„¶åå¯åŠ¨æœåŠ¡å™¨ã€‚
    """
    # è®¾ç½®æ—¥å¿—
    setup_logging()
    logger.info("å¯åŠ¨ Synapse MCP æœåŠ¡å™¨...")
    
    try:
        # è¿è¡ŒFastMCPæœåŠ¡å™¨
        # é»˜è®¤ä½¿ç”¨stdioä¼ è¾“æ–¹å¼ï¼Œè¿™æ˜¯MCPçš„æ ‡å‡†è¿æ¥æ–¹å¼
        mcp.run()
        
    except KeyboardInterrupt:
        logger.info("æ¥æ”¶åˆ°ä¸­æ–­ä¿¡å·ï¼Œæ­£åœ¨å…³é—­æœåŠ¡å™¨...")
    except Exception as e:
        logger.error(f"æœåŠ¡å™¨è¿è¡Œæ—¶é”™è¯¯: {str(e)}", exc_info=True)
        sys.exit(1)

def sync_main():
    """
    Synchronous version of main entry point for compatibility.
    
    This function provides a synchronous interface to start the async server.
    """
    try:
        asyncio.run(main())
    except KeyboardInterrupt:
        print("\nServer stopped")
    except Exception as e:
        print(f"Server startup failed: {str(e)}")
        sys.exit(1)

if __name__ == "__main__":
    sync_main()