"""
Synapse MCP Server - Main Server Module

This module contains the complete implementation of an MCP (Model Context Protocol) server
that provides intelligent memory and knowledge retrieval functionality.

Key Features:
- FastMCP framework-based MCP server
- Tool registration (save-conversation, search-knowledge, inject-context)
- Unified error handling and response formatting
- Parameter validation and server lifecycle management
- MCP protocol compliance for seamless AI assistant integration
"""

import asyncio
import logging
import json
import time
from contextlib import asynccontextmanager
from dataclasses import dataclass
from datetime import datetime
import sys

from mcp.server.fastmcp import FastMCP, Context

from synapse.models import ConversationRecord
from synapse.storage.paths import StoragePaths
from synapse.storage.initializer import StorageInitializer, initialize_synapse_storage
from synapse.storage.file_manager import FileManager
from synapse.utils.logging_config import setup_logging
from synapse.tools.save_conversation import SaveConversationTool
from synapse.tools.search_knowledge import SearchKnowledgeTool
from synapse.tools.inject_context import InjectContextTool
from synapse.tools.extract_solutions import ExtractSolutionsTool

logger = logging.getLogger(__name__)

# Mock Database class for demonstration purposes
class Database:
    """Mock database class for demonstration purposes"""
    
    @classmethod
    async def connect(cls) -> "Database":
        """Connect to database"""
        logger.info("Database connection established")
        return cls()
    
    async def disconnect(self) -> None:
        """Disconnect from database"""
        logger.info("Database connection closed")
    
    async def save_conversation(self, conversation: ConversationRecord) -> str:
        """Save conversation record (mock implementation)"""
        logger.info(f"Saving conversation: {conversation.id}")
        return conversation.id
    
    async def search_conversations(self, query: str, limit: int = 10) -> list:
        """Search conversation records (mock implementation)"""
        logger.info(f"Search query: '{query}', limit: {limit}")
        return [
            {
                "id": "conv_20240115_001",
                "title": f"Solution for '{query}'",
                "snippet": f"This is a detailed explanation and code example for {query}...",
                "match_score": 0.95,
                "created_at": "2024-01-15T10:30:00Z",
                "tags": [query.lower(), "programming"],
                "type": "conversation"
            }
        ]

@dataclass
class AppContext:
    """Application context containing database connections and resources"""
    db: Database
    storage_paths: StoragePaths
    file_manager: FileManager
    save_conversation_tool: SaveConversationTool
    search_knowledge_tool: SearchKnowledgeTool
    inject_context_tool: InjectContextTool
    extract_solutions_tool: ExtractSolutionsTool

@asynccontextmanager
async def app_lifespan(_: FastMCP):
    """Manage application lifecycle with resource initialization and cleanup"""
    logger.info("Starting Synapse MCP Server...")
    
    try:
        # Initialize storage system
        loop = asyncio.get_event_loop()
        await loop.run_in_executor(None, initialize_synapse_storage)
        
        # Connect to database
        db = await Database.connect()
        
        # Create resource managers
        storage_paths = StoragePaths()
        file_manager = FileManager(storage_paths)
        
        # Create tool instances
        save_conversation_tool = SaveConversationTool(storage_paths)
        search_knowledge_tool = SearchKnowledgeTool(storage_paths, file_manager)
        inject_context_tool = InjectContextTool()
        extract_solutions_tool = ExtractSolutionsTool(storage_paths)
        
        # Create application context
        app_context = AppContext(
            db=db, 
            storage_paths=storage_paths,
            file_manager=file_manager,
            save_conversation_tool=save_conversation_tool,
            search_knowledge_tool=search_knowledge_tool,
            inject_context_tool=inject_context_tool,
            extract_solutions_tool=extract_solutions_tool
        )
        
        logger.info("Synapse MCP Server started successfully")
        yield app_context
        
    except Exception as e:
        logger.error(f"Server startup failed: {str(e)}")
        raise
    finally:
        # Cleanup resources
        logger.info("Shutting down Synapse MCP Server...")
        if 'db' in locals():
            await db.disconnect()
        logger.info("Synapse MCP Server shutdown complete")

# Create FastMCP server instance
mcp = FastMCP("synapse-mcp", lifespan=app_lifespan)

# ==================== Internal Analysis Functions ====================

async def _generate_conversation_analysis(title: str, ctx = None) -> dict:
    """
    Generate automatic conversation analysis when AI analysis is not provided.
    
    This function provides a fallback mechanism to ensure save_conversation
    can work even when AI forgets to call the conversation_analysis_prompt.
    
    Args:
        title: Conversation title
        ctx: MCP context for logging
        
    Returns:
        dict: Analysis results with summary, tags, importance, category, solutions
    """
    if ctx:
        await ctx.info("Generating automatic conversation analysis...")
    
    # Basic analysis based on title keywords
    title_lower = title.lower()
    
    # Determine category from title keywords
    category_keywords = {
        "problem-solving": ["bug", "fix", "error", "issue", "problem", "solve", "debug", "troubleshoot"],
        "learning": ["learn", "tutorial", "how to", "understand", "explain", "guide", "lesson"],
        "implementation": ["implement", "create", "build", "develop", "make", "add", "feature"],
        "code-review": ["review", "refactor", "improve", "optimize", "clean", "quality"],
        "debugging": ["debug", "trace", "investigate", "diagnose", "crash", "exception"]
    }
    
    category = "general"
    for cat, keywords in category_keywords.items():
        if any(keyword in title_lower for keyword in keywords):
            category = cat
            break
    
    # Extract basic tags from title
    technical_keywords = [
        "python", "javascript", "java", "cpp", "c++", "react", "vue", "angular", "nodejs", "node.js",
        "django", "flask", "fastapi", "spring", "api", "database", "sql", "mongodb", "redis",
        "docker", "kubernetes", "git", "github", "aws", "azure", "gcp", "linux", "windows",
        "machine learning", "ml", "ai", "data science", "pandas", "numpy", "tensorflow", "pytorch"
    ]
    
    tags = []
    for keyword in technical_keywords:
        if keyword in title_lower:
            tags.append(keyword.replace(" ", "_"))
    
    # Add category as tag if not already present
    if category != "general" and category not in tags:
        tags.append(category.replace("-", "_"))
    
    # Determine importance based on keywords
    high_importance_keywords = ["critical", "urgent", "important", "security", "performance", "production"]
    low_importance_keywords = ["minor", "cosmetic", "simple", "basic"]
    
    if any(keyword in title_lower for keyword in high_importance_keywords):
        importance = 5
    elif any(keyword in title_lower for keyword in low_importance_keywords):
        importance = 2
    else:
        importance = 3
    
    # Generate basic summary
    summary = f"Discussion about {title}. This conversation was automatically analyzed and categorized as {category}."
    if tags:
        summary += f" Related technologies: {', '.join(tags[:3])}."
    
    analysis_result = {
        "summary": summary,
        "tags": tags[:5],  # Limit to 5 tags
        "importance": importance,
        "category": category,
        "solutions": [],  # Basic analysis doesn't extract solutions
        "auto_generated": True
    }
    
    if ctx:
        await ctx.info(f"Auto-analysis completed: category={category}, tags={len(tags)}, importance={importance}")
    
    return analysis_result

# ==================== MCP Prompt Templates ====================

@mcp.prompt(title="Conversation Analysis")
def conversation_analysis_prompt(
    title: str,
    focus: str = "comprehensive"
) -> str:
    """
    Conversation analysis prompt template.
    
    Analyzes the current conversation context automatically without requiring
    explicit content input. The AI will analyze the full conversation history.
    
    Args:
        title: Conversation title for the analysis
        focus: Analysis focus ("comprehensive", "summary", "tags", "solutions")
    
    Returns:
        str: Formatted analysis prompt for current conversation context
    """
    base_prompt = f"""Please analyze the current technical conversation and provide structured analysis results.

## Conversation Information
**Title**: {title}
**Content**: Please analyze the complete current conversation context

## Analysis Requirements
Please provide the following analysis results in JSON format:

```json
{{
    "summary": "Concise summary of the current conversation (1-2 sentences)",
    "tags": ["technical_tag1", "technical_tag2", "framework_name", "..."],
    "importance": 3,
    "category": "problem-solving | learning | code-review | implementation | debugging | general",
    "has_solutions": true,
    "solution_indicators": ["Types of solutions or approaches discussed"]
}}
```

## Analysis Focus"""
    
    if focus == "comprehensive":
        return base_prompt + """
- Comprehensive analysis: summary, tags, importance, category, solution identification
- Identify tech stack, programming languages, frameworks
- Evaluate educational value and practical utility
- Detect reusable solutions"""
    elif focus == "summary":
        return base_prompt + """
- Focus on generating accurate and concise summaries
- Extract core technical points and conclusions"""
    elif focus == "tags":
        return base_prompt + """
- Focus on extracting technical tags
- Identify programming languages, frameworks, tools, concepts
- Sort by relevance, maximum 10 tags"""
    elif focus == "solutions":
        return base_prompt + """
- Focus on identifying reusable solutions
- Detect code snippets, configuration examples, best practices
- Evaluate solution generalizability"""
    else:
        return base_prompt

# ==================== MCP Tool Implementations ====================

@mcp.tool()
async def save_conversation(
    title: str,
    tags: list[str] = None,
    category: str = None, 
    importance: int = None,
    check_duplicates: bool = True,
    # AIÂàÜÊûêÁªìÊûúÂèÇÊï∞ÔºàÈÄöËøáconversation_analysis_promptËé∑ÂæóÂêé‰º†ÂÖ•Ôºâ
    ai_summary: str = None,
    ai_tags: list[str] = None,
    ai_importance: int = None,
    ai_category: str = None,
    ai_solutions: list[dict] = None,
    ctx: Context = None
) -> dict:
    """
    Save AI conversation records to knowledge base with automatic analysis.
    
    This tool automatically handles conversation analysis and storage:
    - Auto-generates analysis if AI analysis results are not provided
    - Accepts user-specified or AI-analyzed metadata  
    - Extracts content from current conversation context automatically
    - Detects and notifies duplicate conversations
    - Updates search indexes for fast queries
    
    Usage workflows:
    1. Simple: User says "‰øùÂ≠òÂØπËØù" ‚Üí AI calls save_conversation(title="...") ‚Üí Auto-analysis generated
    2. Advanced: AI calls conversation_analysis_prompt first, then passes results to save_conversation
    
    Args:
        title: Conversation topic title (required)
        tags: User-defined tag list (optional)
        category: User-specified conversation category (optional)
        importance: User-specified importance level 1-5 (optional)
        check_duplicates: Whether to check for duplicate conversations (default True)
        ai_summary: AI-generated summary (optional - will auto-generate if not provided)
        ai_tags: AI-extracted tag list (optional - will auto-generate if not provided)  
        ai_importance: AI-assessed importance (optional - will auto-generate if not provided)
        ai_category: AI-inferred category (optional - will auto-generate if not provided)
        ai_solutions: AI-extracted solution list (optional - will auto-generate if not provided)
        ctx: MCP context object for progress reporting
        
    Returns:
        dict: Detailed save result information including:
            - success: Whether save was successful
            - conversation: Saved conversation details
            - duplicates_found: Number of duplicate conversations found
            - duplicate_ids: List of duplicate conversation IDs
            - storage_path: File storage path
    """
    try:
        if ctx:
            await ctx.info(f"Starting conversation save: {title}")
        
        # Basic parameter validation
        if not title or not title.strip():
            raise ValueError("Conversation title cannot be empty")
        
        # Auto-generate analysis if not provided
        if not ai_summary:
            if ctx:
                await ctx.info("AI analysis not provided, auto-generating conversation analysis...")
            
            # Generate automatic analysis using built-in logic
            auto_analysis = await _generate_conversation_analysis(title, ctx)
            ai_summary = auto_analysis.get("summary", f"Conversation about: {title}")
            ai_tags = auto_analysis.get("tags", [])
            ai_importance = auto_analysis.get("importance", 3)
            ai_category = auto_analysis.get("category", "general")
            ai_solutions = auto_analysis.get("solutions", [])
            auto_analysis_used = True
        else:
            auto_analysis_used = False
        
        if importance is not None and (importance < 1 or importance > 5):
            raise ValueError("Importance level must be between 1-5")
        
        # Get save conversation tool instance
        save_tool = None
        if ctx and hasattr(ctx, 'request_context') and hasattr(ctx.request_context, 'lifespan_context'):
            save_tool = ctx.request_context.lifespan_context.save_conversation_tool
        
        if not save_tool:
            raise RuntimeError("Êó†Ê≥ïËé∑ÂèñSaveConversationToolÂÆû‰æãÔºåËØ∑Ê£ÄÊü•ÊúçÂä°Âô®ÈÖçÁΩÆ")
        
        if ctx:
            await ctx.info("Processing conversation with AI analysis results...")
        
        # Create conversation content from AI summary since we no longer require explicit content
        # The AI has already analyzed the complete conversation context
        conversation_content = ai_summary + "\n\n[Full conversation content was analyzed by AI system]"
        
        # ‰ΩøÁî®SaveConversationToolËøõË°å‰øùÂ≠ò
        result = await save_tool.save_conversation(
            title=title.strip(),
            content=conversation_content,
            user_tags=tags,
            user_category=category,
            user_importance=importance,
            check_duplicates=check_duplicates,
            # ‰º†ÈÄíAIÂàÜÊûêÁªìÊûú
            ai_summary=ai_summary,
            ai_tags=ai_tags,
            ai_importance=ai_importance,
            ai_category=ai_category,
            ai_solutions=ai_solutions,
            ctx=ctx
        )
        
        # Ê£ÄÊü•‰øùÂ≠òÁªìÊûú
        if not result.get("success", False):
            error_msg = result.get("error", "Êú™Áü•ÈîôËØØ")
            raise RuntimeError(f"‰øùÂ≠òÂ§±Ë¥•: {error_msg}")
        
        conversation_info = result.get("conversation", {})
        duplicates_count = result.get("duplicates_found", 0)
        
        if ctx:
            if duplicates_count > 0:
                await ctx.info(f"Ê£ÄÊµãÂà∞ {duplicates_count} ‰∏™Áõ∏‰ººÂØπËØù")
            
            await ctx.info(f"ÂØπËØù‰øùÂ≠òÊàêÂäü: {conversation_info.get('id', 'Unknown')}")
            
            if auto_analysis_used:
                await ctx.info(f"‰ΩøÁî®Ëá™Âä®ÂàÜÊûêÁîüÊàê‰∫ÜÊëòË¶ÅÂíå {len(ai_tags)} ‰∏™Ê†áÁ≠æ")
            else:
                await ctx.info(f"‰ΩøÁî®AIÊèê‰æõÁöÑÂàÜÊûêÁªìÊûúÔºåÂåÖÂê´ {conversation_info.get('auto_tags_count', 0)} ‰∏™Ê†áÁ≠æ")
        
        # ÊûÑÂª∫ËøîÂõûÁªìÊûúÔºà‰∏éÂÖ∂‰ªñÂ∑•ÂÖ∑È£éÊ†º‰øùÊåÅ‰∏ÄËá¥Ôºâ
        return {
            "success": True,
            "conversation": {
                "id": conversation_info.get("id"),
                "title": conversation_info.get("title"),
                "summary": conversation_info.get("summary", ""),
                "category": conversation_info.get("category", "general"),
                "importance": conversation_info.get("importance", 3),
                "tags": conversation_info.get("tags", []),
                "created_at": conversation_info.get("created_at"),
                "searchable": True
            },
            "analysis": {
                "auto_generated": auto_analysis_used,
                "method": "Ëá™Âä®ÂàÜÊûê" if auto_analysis_used else "AIÊèê‰æõÂàÜÊûê",
                "tags_extracted": conversation_info.get("auto_tags_count", 0),
                "user_tags_added": conversation_info.get("user_tags_count", 0),
                "solutions_found": conversation_info.get("solutions_count", 0)
            },
            "duplicates": {
                "checked": check_duplicates,
                "found": duplicates_count,
                "similar_ids": result.get("duplicate_ids", [])
            },
            "storage": {
                "path": result.get("storage_path"),
                "indexed": True,
                "status": "Â∑≤‰øùÂ≠ò"
            }
        }
        
    except Exception as e:
        error_msg = str(e)
        if ctx:
            await ctx.error(f"‰øùÂ≠òÂØπËØùÂ§±Ë¥•: {error_msg}")
        
        logger.error(f"‰øùÂ≠òÂØπËØùÂ§±Ë¥• - Ê†áÈ¢ò: {title[:50]}..., ÈîôËØØ: {error_msg}", exc_info=True)
        
        # ËøîÂõûÈîôËØØ‰ø°ÊÅØËÄå‰∏çÊòØÊäõÂá∫ÂºÇÂ∏∏ÔºåËÆ©Ë∞ÉÁî®ÊñπËÉΩÂ§üÂ§ÑÁêÜ
        return {
            "success": False,
            "error": error_msg,
            "error_type": type(e).__name__,
            "conversation": None,
            "analysis": {
                "auto_generated": False,
                "method": "ÂàÜÊûêÂ§±Ë¥•",
                "tags_extracted": 0,
                "user_tags_added": 0,
                "solutions_found": 0
            }
        }

@mcp.tool()
async def search_knowledge(
    query: str,
    search_in: str = "all",
    limit: int = 10,
    ctx: Context = None
) -> dict:
    """
    ÁÆÄÂçïgrepÊêúÁ¥¢Â∑•ÂÖ∑ - ËÆ©AIÊèê‰æõÂÖ≥ÈîÆËØçÔºåÂ∑•ÂÖ∑Ë¥üË¥£ÊêúÁ¥¢
    
    ü§ñ AI‰ΩøÁî®Âª∫ËÆÆÔºö
    - Ê†πÊçÆÁî®Êà∑ÈóÆÈ¢òÁîüÊàêÂ§ö‰∏™‰∏çÂêåÁöÑÂÖ≥ÈîÆËØçËøõË°åÊêúÁ¥¢
    - Â∞ùËØï‰∏≠Ëã±Êñá„ÄÅÊäÄÊúØÊúØËØ≠ÁöÑ‰∏çÂêåË°®ËææÊñπÂºè
    - Âª∫ËÆÆËøûÁª≠ÊêúÁ¥¢ 2-3 Ê¨°‰∏çÂêåÂÖ≥ÈîÆËØç‰ª•ÊèêÈ´òÂè¨ÂõûÁéá
    
    Á§∫‰æãÁî®Ê≥ïÔºö
    Áî®Êà∑ÈóÆÔºö"PythonÂºÇÊ≠•ÁºñÁ®ãÈîôËØØÂ§ÑÁêÜ"
    AIÂ∫îËØ•ÊêúÁ¥¢Ôºö
    1. search_knowledge("Python async exception")
    2. search_knowledge("ÂºÇÊ≠• ÈîôËØØÂ§ÑÁêÜ")
    3. search_knowledge("asyncio try except")
    
    Args:
        query: ÊêúÁ¥¢ÂÖ≥ÈîÆËØçÔºàÁî±AIÁêÜËß£Áî®Êà∑ÈóÆÈ¢òÂêéÁîüÊàêÔºâÔºàÂøÖÈúÄÔºâ
        search_in: ÊêúÁ¥¢ËåÉÂõ¥ ("title", "content", "tags", "all")
        limit: ËøîÂõûÁªìÊûúÊï∞Èáè (1-50)
        ctx: MCP‰∏ä‰∏ãÊñáÂØπË±°
        
    Returns:
        dict: ÊêúÁ¥¢ÁªìÊûúÂíåAIÂêéÁª≠ÊêúÁ¥¢Âª∫ËÆÆ
        {
            "query": "Áî®Êà∑ËæìÂÖ•ÁöÑÂÖ≥ÈîÆËØç",
            "total_found": 5,
            "search_area": "all",
            "results": [
                {
                    "id": "sol_001",
                    "title": "...",
                    "snippet": "ÂåπÈÖçÂÜÖÂÆπÁâáÊÆµ...",
                    "created_at": "2024-01-01T12:00:00Z",
                    "match_reason": "Ê†áÈ¢òÂåπÈÖç 'async'"
                }
            ],
            "suggestion": "Âª∫ËÆÆAIÂ∞ùËØïÊêúÁ¥¢ÂÖ∂‰ªñÁõ∏ÂÖ≥ÂÖ≥ÈîÆËØçÂ¶Ç 'asyncio', 'ÂºÇÊ≠•ÁºñÁ®ã'"
        }
    """
    try:
        if ctx:
            await ctx.info(f"ÂºÄÂßãAIËØ≠‰πâÊêúÁ¥¢: '{query}'")
        
        # Ëé∑ÂèñÊêúÁ¥¢Áü•ËØÜÂ∑•ÂÖ∑ÂÆû‰æã
        search_tool = None
        if ctx and hasattr(ctx, 'request_context') and hasattr(ctx.request_context, 'lifespan_context'):
            search_tool = ctx.request_context.lifespan_context.search_knowledge_tool
        
        if not search_tool:
            raise RuntimeError("Êó†Ê≥ïËé∑ÂèñSearchKnowledgeToolÂÆû‰æãÔºåËØ∑Ê£ÄÊü•ÊúçÂä°Âô®ÈÖçÁΩÆ")
        
        if ctx:
            await ctx.info("ÊâßË°åÁÆÄÂçïÊñáÊú¨ÊêúÁ¥¢...")
        
        # ‰ΩøÁî®ÁÆÄÂåñÁöÑSearchKnowledgeToolËøõË°ågrepÊêúÁ¥¢
        result = search_tool.search_knowledge(
            query=query.strip(),
            search_in=search_in
        )
        
        # Ê£ÄÊü•ÊêúÁ¥¢ÁªìÊûú
        if not result:
            raise RuntimeError("ÊêúÁ¥¢Â∑•ÂÖ∑ËøîÂõûÁ©∫ÁªìÊûú")
        
        processing_time = result.get("processing_time_ms", 0)
        total_found = result.get("total_found", 0)
        
        if ctx:
            await ctx.info(f"grepÊêúÁ¥¢ÂÆåÊàê: ÊâæÂà∞ {total_found} ‰∏™ÁªìÊûúÔºåËÄóÊó∂ {processing_time:.2f}ms")
            if result.get("suggestion"):
                await ctx.info(f"AIÂª∫ËÆÆ: {result['suggestion']}")
        
        # ËøîÂõûÁÆÄÂåñÁöÑÊêúÁ¥¢ÁªìÊûú
        return result
        
    except Exception as e:
        error_msg = str(e)
        if ctx:
            await ctx.error(f"ÁÆÄÂåñgrepÊêúÁ¥¢Â§±Ë¥•: {error_msg}")
        
        logger.error(f"ÁÆÄÂåñgrepÊêúÁ¥¢Â§±Ë¥• - Êü•ËØ¢: '{query}', ÈîôËØØ: {error_msg}", exc_info=True)
        
        # ËøîÂõûÈîôËØØ‰ø°ÊÅØ
        return {
            "query": query,
            "total_found": 0,
            "search_area": search_in,
            "results": [],
            "error": error_msg,
            "processing_time_ms": 0
        }


@mcp.tool()
async def inject_context(
    current_query: str,
    search_results: list,
    include_solutions: bool = True,
    include_conversations: bool = True,
    ctx: Context = None
) -> dict:
    """
    ÂêëÂΩìÂâçÂØπËØùÊ≥®ÂÖ•ÊêúÁ¥¢ÁªìÊûú‰∏ä‰∏ãÊñáÔºåÂπ∂ÂºïÂØºAIÂ∫îÁî®Âà∞ÈóÆÈ¢òËß£ÂÜ≥‰∏≠
    
    Ëøô‰∏™Â∑•ÂÖ∑ÈááÁî®ÁÆÄÂåñÁ≠ñÁï•ÔºåÁõ¥Êé•Êé•Êî∂ÊêúÁ¥¢ÁªìÊûúÂπ∂ÂºïÂØºAIËøõË°åÊô∫ËÉΩÂ∫îÁî®Ôºö
    - Êé•Êî∂Êù•Ëá™search_knowledgeÁöÑÊêúÁ¥¢ÁªìÊûú
    - ÁÆÄÂçïÊ†ºÂºèÂåñÂ§ÑÁêÜÔºå‰øùÁïôÂÖ≥ÈîÆ‰ø°ÊÅØ
    - ËøîÂõûÁªìÊûÑÂåñÊï∞ÊçÆÂºïÂØºAIÂ∫îÁî®Ëß£ÂÜ≥ÊñπÊ°à
    - ËÆ©AIË¥üË¥£ËØ≠‰πâÁêÜËß£ÂíåÂÖ∑‰ΩìÂÆûÊñΩ
    
    Êé®Ëçê‰ΩøÁî®ÊµÅÁ®ãÔºö
    1. ÂÖàË∞ÉÁî®search_knowledgeËé∑ÂèñÁõ∏ÂÖ≥ÂÜÖÂÆπ
    2. Â∞ÜÊêúÁ¥¢ÁªìÊûú‰º†ÂÖ•Ê≠§Â∑•ÂÖ∑ËøõË°å‰∏ä‰∏ãÊñáÊ≥®ÂÖ•
    3. AIÊ†πÊçÆËøîÂõûÁöÑÊåáÂØºËá™Âä®Â∫îÁî®Ëß£ÂÜ≥ÊñπÊ°à
    
    Args:
        current_query: ÂΩìÂâçÁî®Êà∑ÁöÑÊü•ËØ¢ÊàñÈóÆÈ¢òÔºàÂøÖÈúÄÔºâ
        search_results: Êù•Ëá™search_knowledgeÁöÑÊêúÁ¥¢ÁªìÊûúÂàóË°®ÔºàÂøÖÈúÄÔºâ
        include_solutions: ‰øùÁïôÂêëÂêéÂÖºÂÆπÊÄßÔºàÈªòËÆ§TrueÔºâ
        include_conversations: ‰øùÁïôÂêëÂêéÂÖºÂÆπÊÄßÔºàÈªòËÆ§TrueÔºâ
        ctx: MCP‰∏ä‰∏ãÊñáÂØπË±°
        
    Returns:
        dict: ÂºïÂØºAIËøõË°åÈóÆÈ¢òËß£ÂÜ≥Â∫îÁî®ÁöÑÁªìÊûÑÂåñÊï∞ÊçÆ
        {
            "content": [{"type": "text", "text": "ÁªìÊûÑÂåñJSONÊï∞ÊçÆ"}],
            "processing_time_ms": float,
            "total_items": int,
            "injection_summary": str
        }
    """
    try:
        if ctx:
            await ctx.info(f"ÂºÄÂßãÊô∫ËÉΩ‰∏ä‰∏ãÊñáÊ≥®ÂÖ•: '{current_query[:50]}'")
        
        # Âü∫Á°ÄÂèÇÊï∞È™åËØÅ
        if not current_query or not current_query.strip():
            raise ValueError("ÂΩìÂâçÊü•ËØ¢‰∏çËÉΩ‰∏∫Á©∫")
        
        if not isinstance(search_results, list):
            raise ValueError("search_results ÂøÖÈ°ªÊòØÂàóË°®Á±ªÂûã")
        
        # Ëé∑Âèñ‰∏ä‰∏ãÊñáÊ≥®ÂÖ•Â∑•ÂÖ∑ÂÆû‰æã
        inject_tool = None
        if ctx and hasattr(ctx, 'request_context') and hasattr(ctx.request_context, 'lifespan_context'):
            inject_tool = ctx.request_context.lifespan_context.inject_context_tool
        
        if not inject_tool:
            raise RuntimeError("Êó†Ê≥ïËé∑ÂèñInjectContextToolÂÆû‰æãÔºåËØ∑Ê£ÄÊü•ÊúçÂä°Âô®ÈÖçÁΩÆ")
        
        if ctx:
            await ctx.info(f"Â§ÑÁêÜ {len(search_results)} ‰∏™ÊêúÁ¥¢ÁªìÊûú...")
        
        # ‰ΩøÁî®InjectContextToolËøõË°åÊô∫ËÉΩ‰∏ä‰∏ãÊñáÊ≥®ÂÖ•
        result = await inject_tool.inject_context(
            current_query=current_query.strip(),
            search_results=search_results,
            include_solutions=include_solutions,
            include_conversations=include_conversations,
            ctx=ctx
        )
        
        # Ê£ÄÊü•Ê≥®ÂÖ•ÁªìÊûú
        if not result:
            raise RuntimeError("‰∏ä‰∏ãÊñáÊ≥®ÂÖ•Â∑•ÂÖ∑ËøîÂõûÁ©∫ÁªìÊûú")
        
        context_items_count = result.get("total_items", 0)
        processing_time = result.get("processing_time_ms", 0)
        
        if ctx:
            if context_items_count > 0:
                await ctx.info(f"ÊàêÂäüÊ≥®ÂÖ• {context_items_count} ‰∏™Áõ∏ÂÖ≥‰∏ä‰∏ãÊñáÈ°π")
                
                # Êèê‰æõËØ¶ÁªÜÁöÑÊ≥®ÂÖ•ÁªüËÆ°‰ø°ÊÅØ
                stats = result.get("search_statistics", {})
                if stats:
                    candidates = stats.get("candidates_found", 0)
                    above_threshold = stats.get("above_threshold", 0)
                    if candidates > 0:
                        await ctx.info(f"Ê≥®ÂÖ•ÁªüËÆ°: {candidates} ‰∏™ÂÄôÈÄâ ‚Üí {above_threshold} ‰∏™Á¨¶ÂêàÈòàÂÄº ‚Üí {context_items_count} ‰∏™ÊúÄÁªàÈÄâÊã©")
                
                # ÊÄßËÉΩ‰ø°ÊÅØ
                if processing_time > 300:
                    await ctx.info(f"Â§ÑÁêÜÊó∂Èó¥: {processing_time:.2f}ms (ÁõÆÊ†á<500ms)")
                else:
                    await ctx.info(f"Â§ÑÁêÜÈ´òÊïà: {processing_time:.2f}ms")
            else:
                await ctx.info(f"Êú™ÊâæÂà∞Á¨¶ÂêàÈòàÂÄº {relevance_threshold} ÁöÑÁõ∏ÂÖ≥‰∏ä‰∏ãÊñá")
        
        # ËøîÂõûÁÆÄÂåñÁöÑÁªìÊûúÊ†ºÂºè
        return {
            "content": result.get("content", []),
            "injection_summary": result.get("injection_summary", "‰∏ä‰∏ãÊñáÊ≥®ÂÖ•ÂÆåÊàê"),
            "total_items": context_items_count,
            "query": current_query,
            "processing_time_ms": round(processing_time, 2)
        }
        
    except Exception as e:
        error_msg = str(e)
        if ctx:
            await ctx.error(f"‰∏ä‰∏ãÊñáÊ≥®ÂÖ•Â§±Ë¥•: {error_msg}")
        
        logger.error(f"‰∏ä‰∏ãÊñáÊ≥®ÂÖ•Â§±Ë¥• - Êü•ËØ¢: '{current_query}', ÈîôËØØ: {error_msg}", exc_info=True)
        
        # ËøîÂõûÈîôËØØ‰ø°ÊÅØËÄå‰∏çÊòØÊäõÂá∫ÂºÇÂ∏∏ÔºåËÆ©Ë∞ÉÁî®ÊñπËÉΩÂ§üÂ§ÑÁêÜ
        return {
            "content": [],
            "injection_summary": f"Ê≥®ÂÖ•Â§±Ë¥•: {error_msg}",
            "total_items": 0,
            "query": current_query,
            "processing_time_ms": 0,
            "error": error_msg
        }

@mcp.tool()
async def export_data(
    export_path: str,
    include_backups: bool = False,
    include_cache: bool = False,
    ctx: Context = None
) -> dict:
    """
    Export all Synapse data to a specified directory path.
    
    This tool exports conversations, solutions, indexes, and optionally backups
    to enable data migration, backup, and sharing between systems.
    
    Args:
        export_path: Target directory path for exported data (required)
        include_backups: Whether to include backup files in export (default: False)
        include_cache: Whether to include cache files in export (default: False)
        ctx: MCP context object for logging and progress reporting
        
    Returns:
        dict: Export result with status, statistics, and file paths
    """
    try:
        if ctx:
            await ctx.info(f"Starting data export to: {export_path}")
        
        # Parameter validation
        if not export_path or not export_path.strip():
            raise ValueError("Export path cannot be empty")
        
        from pathlib import Path
        export_dir = Path(export_path.strip()).expanduser().resolve()
        
        # Check if export path is valid and writable
        if export_dir.exists() and not export_dir.is_dir():
            raise ValueError(f"Export path exists but is not a directory: {export_dir}")
        
        # Get file manager instance
        file_manager = None
        if ctx and hasattr(ctx, 'request_context') and hasattr(ctx.request_context, 'lifespan_context'):
            file_manager = ctx.request_context.lifespan_context.file_manager
        
        if not file_manager:
            raise RuntimeError("Êó†Ê≥ïËé∑ÂèñFileManagerÂÆû‰æãÔºåËØ∑Ê£ÄÊü•ÊúçÂä°Âô®ÈÖçÁΩÆ")
        
        if ctx:
            await ctx.info("Validating export directory permissions...")
        
        # Test write permissions by creating export directory
        try:
            export_dir.mkdir(parents=True, exist_ok=True)
        except PermissionError:
            raise PermissionError(f"No write permission for export path: {export_dir}")
        except Exception as e:
            raise RuntimeError(f"Failed to create export directory: {e}")
        
        if ctx:
            await ctx.info("Gathering storage statistics...")
        
        # Get pre-export statistics
        storage_stats = file_manager.get_storage_statistics()
        
        if ctx:
            await ctx.info(f"Found {storage_stats.total_conversations} conversations and {storage_stats.total_solutions} solutions")
            await ctx.report_progress(progress=0.2, message="Starting data export...")
        
        # Perform the export using FileManager
        export_success = file_manager.export_data(export_dir, include_backups=include_backups)
        
        if not export_success:
            raise RuntimeError("Export operation failed - check file manager logs for details")
        
        if ctx:
            await ctx.report_progress(progress=0.8, message="Finalizing export...")
        
        if ctx:
            await ctx.report_progress(progress=1.0, message="Export completed successfully")
            await ctx.info(f"Exported {storage_stats.total_files} files ({storage_stats.disk_usage_mb:.2f} MB) to {export_dir}")
        
        # Build comprehensive response
        result = {
            "success": True,
            "export_path": str(export_dir),
            "exported_at": datetime.now().isoformat(),
            "statistics": {
                "total_conversations_exported": storage_stats.total_conversations,
                "total_solutions_exported": storage_stats.total_solutions,
                "total_files_exported": storage_stats.total_files,
                "total_size_bytes": storage_stats.total_size_bytes,
                "total_size_mb": round(storage_stats.disk_usage_mb, 2),
                "include_backups": include_backups,
                "include_cache": include_cache
            },
            "exported_components": {
                "conversations": True,
                "solutions": True,
                "indexes": True,
                "backups": include_backups,
                "cache": include_cache,
                "export_info": True
            },
            "summary": f"Successfully exported {storage_stats.total_conversations} conversations and {storage_stats.total_solutions} solutions to {export_dir}"
        }
        
        return result
        
    except Exception as e:
        error_msg = str(e)
        if ctx:
            await ctx.error(f"Data export failed: {error_msg}")
        
        logger.error(f"Êï∞ÊçÆÂØºÂá∫Â§±Ë¥• - Ë∑ØÂæÑ: {export_path}, ÈîôËØØ: {error_msg}", exc_info=True)
        
        return {
            "success": False,
            "export_path": export_path,
            "error": error_msg,
            "error_type": type(e).__name__,
            "exported_at": datetime.now().isoformat(),
            "statistics": {
                "total_conversations_exported": 0,
                "total_solutions_exported": 0,
                "total_files_exported": 0,
                "total_size_bytes": 0,
                "total_size_mb": 0,
                "include_backups": include_backups,
                "include_cache": include_cache
            },
            "summary": f"Export failed: {error_msg}"
        }

@mcp.tool()
async def import_data(
    import_path: str,
    merge_mode: str = "append",
    validate_data: bool = True,
    create_backup: bool = True,
    ctx: Context = None
) -> dict:
    """
    Import Synapse data from a specified directory path.
    
    This tool imports conversations, solutions, and indexes from an exported
    data directory, with options for data validation and merge strategies.
    
    Args:
        import_path: Source directory path containing exported data (required)
        merge_mode: Merge strategy - "append" or "overwrite" (default: "append")
        validate_data: Whether to validate data format before import (default: True)
        create_backup: Whether to create backup before import (default: True)
        ctx: MCP context object for logging and progress reporting
        
    Returns:
        dict: Import result with status, statistics, and validation results
    """
    try:
        if ctx:
            await ctx.info(f"Starting data import from: {import_path}")
        
        # Parameter validation
        if not import_path or not import_path.strip():
            raise ValueError("Import path cannot be empty")
        
        if merge_mode not in ["append", "overwrite"]:
            raise ValueError("Merge mode must be 'append' or 'overwrite'")
        
        from pathlib import Path
        import_dir = Path(import_path.strip()).expanduser().resolve()
        
        # Check if import path exists and is valid
        if not import_dir.exists():
            raise FileNotFoundError(f"Import path does not exist: {import_dir}")
        
        if not import_dir.is_dir():
            raise ValueError(f"Import path is not a directory: {import_dir}")
        
        # Get file manager instance
        file_manager = None
        if ctx and hasattr(ctx, 'request_context') and hasattr(ctx.request_context, 'lifespan_context'):
            file_manager = ctx.request_context.lifespan_context.file_manager
        
        if not file_manager:
            raise RuntimeError("Êó†Ê≥ïËé∑ÂèñFileManagerÂÆû‰æãÔºåËØ∑Ê£ÄÊü•ÊúçÂä°Âô®ÈÖçÁΩÆ")
        
        if ctx:
            await ctx.info("Validating import data structure...")
            await ctx.report_progress(progress=0.1, message="Validating import data...")
        
        # Validate import structure
        validation_results = {
            "conversations_found": False,
            "solutions_found": False,
            "indexes_found": False,
            "export_info_found": False,
            "data_valid": True,
            "validation_errors": []
        }
        
        # Check for expected directories and files
        conversations_dir = import_dir / "conversations"
        solutions_dir = import_dir / "solutions"
        indexes_dir = import_dir / "indexes"
        export_info_file = import_dir / "export_info.json"
        
        validation_results["conversations_found"] = conversations_dir.exists() and conversations_dir.is_dir()
        validation_results["solutions_found"] = solutions_dir.exists() and solutions_dir.is_dir()
        validation_results["indexes_found"] = indexes_dir.exists() and indexes_dir.is_dir()
        validation_results["export_info_found"] = export_info_file.exists() and export_info_file.is_file()
        
        # Basic structure validation
        if not any([validation_results["conversations_found"], validation_results["solutions_found"], validation_results["indexes_found"]]):
            validation_results["data_valid"] = False
            validation_results["validation_errors"].append("No valid data directories found (conversations, solutions, or indexes)")
        
        # Data format validation if requested
        if validate_data and validation_results["data_valid"]:
            if ctx:
                await ctx.info("Performing detailed data validation...")
            
            # Count and validate files
            file_counts = {"conversations": 0, "solutions": 0, "invalid_files": 0}
            
            try:
                # Check conversation files
                if conversations_dir.exists():
                    for conv_file in conversations_dir.rglob("*.json"):
                        if conv_file.is_file():
                            try:
                                with open(conv_file, 'r', encoding='utf-8') as f:
                                    conv_data = json.load(f)
                                    # Basic structure validation
                                    if not all(key in conv_data for key in ["id", "title", "content"]):
                                        file_counts["invalid_files"] += 1
                                        validation_results["validation_errors"].append(f"Invalid conversation format: {conv_file.name}")
                                    else:
                                        file_counts["conversations"] += 1
                            except Exception as e:
                                file_counts["invalid_files"] += 1
                                validation_results["validation_errors"].append(f"Cannot read conversation file {conv_file.name}: {str(e)}")
                
                # Check solution files
                if solutions_dir.exists():
                    for sol_file in solutions_dir.glob("*.json"):
                        if sol_file.is_file():
                            try:
                                with open(sol_file, 'r', encoding='utf-8') as f:
                                    sol_data = json.load(f)
                                    # Basic structure validation
                                    if not all(key in sol_data for key in ["id", "type", "content"]):
                                        file_counts["invalid_files"] += 1
                                        validation_results["validation_errors"].append(f"Invalid solution format: {sol_file.name}")
                                    else:
                                        file_counts["solutions"] += 1
                            except Exception as e:
                                file_counts["invalid_files"] += 1
                                validation_results["validation_errors"].append(f"Cannot read solution file {sol_file.name}: {str(e)}")
                
                if file_counts["invalid_files"] > 0:
                    validation_results["data_valid"] = False
                    
            except Exception as e:
                validation_results["data_valid"] = False
                validation_results["validation_errors"].append(f"Data validation failed: {str(e)}")
        
        if ctx:
            if validation_results["data_valid"]:
                await ctx.info(f"Validation passed - Found {file_counts.get('conversations', 0)} conversations and {file_counts.get('solutions', 0)} solutions")
            else:
                await ctx.info(f"Validation found {len(validation_results['validation_errors'])} issues")
        
        # Proceed with import if validation passes or validation is disabled
        if not validate_data or validation_results["data_valid"]:
            
            # Create backup if requested
            backup_info = None
            if create_backup:
                if ctx:
                    await ctx.info("Creating pre-import backup...")
                    await ctx.report_progress(progress=0.3, message="Creating backup...")
                
                try:
                    backup_timestamp = int(datetime.now().timestamp())
                    backup_path = file_manager.storage_paths.get_backups_dir() / f"pre_import_backup_{backup_timestamp}"
                    backup_success = file_manager.export_data(backup_path, include_backups=False)
                    
                    if backup_success:
                        backup_info = str(backup_path)
                        if ctx:
                            await ctx.info(f"Backup created successfully: {backup_path}")
                    else:
                        logger.warning("Pre-import backup failed, continuing with import")
                        
                except Exception as e:
                    logger.warning(f"Pre-import backup failed: {e}")
            
            if ctx:
                await ctx.info(f"Starting import with merge mode: {merge_mode}")
                await ctx.report_progress(progress=0.5, message="Importing data...")
            
            # Perform the import using FileManager
            import_success = file_manager.import_data(import_dir, merge_mode=merge_mode)
            
            if not import_success:
                raise RuntimeError("Import operation failed - check file manager logs for details")
            
            if ctx:
                await ctx.report_progress(progress=0.9, message="Finalizing import...")
            
            # Get post-import statistics
            post_import_stats = file_manager.get_storage_statistics()
            
            if ctx:
                await ctx.report_progress(progress=1.0, message="Import completed successfully")
                await ctx.info(f"Import completed - Total: {post_import_stats.total_conversations} conversations, {post_import_stats.total_solutions} solutions")
            
            # Build success response
            result = {
                "success": True,
                "import_path": str(import_dir),
                "imported_at": datetime.now().isoformat(),
                "merge_mode": merge_mode,
                "validation_performed": validate_data,
                "backup_created": create_backup,
                "backup_location": backup_info,
                "validation_results": validation_results,
                "statistics": {
                    "conversations_after_import": post_import_stats.total_conversations,
                    "solutions_after_import": post_import_stats.total_solutions,
                    "total_files_after_import": post_import_stats.total_files,
                    "total_size_mb_after_import": round(post_import_stats.disk_usage_mb, 2)
                },
                "imported_components": {
                    "conversations": validation_results["conversations_found"],
                    "solutions": validation_results["solutions_found"],
                    "indexes": validation_results["indexes_found"]
                },
                "summary": f"Successfully imported data from {import_dir} using {merge_mode} mode"
            }
            
            return result
        
        else:
            # Validation failed
            raise ValueError(f"Data validation failed: {', '.join(validation_results['validation_errors'])}")
        
    except Exception as e:
        error_msg = str(e)
        if ctx:
            await ctx.error(f"Data import failed: {error_msg}")
        
        logger.error(f"Êï∞ÊçÆÂØºÂÖ•Â§±Ë¥• - Ë∑ØÂæÑ: {import_path}, ÈîôËØØ: {error_msg}", exc_info=True)
        
        return {
            "success": False,
            "import_path": import_path,
            "error": error_msg,
            "error_type": type(e).__name__,
            "imported_at": datetime.now().isoformat(),
            "merge_mode": merge_mode,
            "validation_performed": validate_data,
            "backup_created": create_backup,
            "validation_results": validation_results if 'validation_results' in locals() else {},
            "summary": f"Import failed: {error_msg}"
        }

@mcp.tool()
async def get_storage_info(ctx: Context = None) -> dict:
    """
    Get comprehensive storage system information and statistics.
    
    This tool provides detailed information about Synapse storage locations,
    directory status, usage statistics, and system health.
    
    Args:
        ctx: MCP context object for logging and progress reporting
        
    Returns:
        dict: Complete storage system information including:
            - Storage directory paths
            - Directory existence and permissions status
            - Storage usage statistics
            - Initialization status
            - System health information
    """
    try:
        if ctx:
            await ctx.info("Retrieving storage system information")
        
        # Initialize storage system components
        storage_paths = StoragePaths()
        initializer = StorageInitializer(storage_paths)
        
        # Get file manager instance for statistics
        file_manager = None
        if ctx and hasattr(ctx, 'request_context') and hasattr(ctx.request_context, 'lifespan_context'):
            file_manager = ctx.request_context.lifespan_context.file_manager
        
        # Get comprehensive storage status
        storage_status = initializer.get_storage_status()
        
        # Calculate storage size in MB
        total_size_mb = storage_status["total_size_bytes"] / (1024 * 1024) if storage_status["total_size_bytes"] > 0 else 0.0
        
        # Get real storage statistics if file manager is available
        storage_stats = None
        conversation_count = 0
        solution_count = 0
        
        if file_manager:
            if ctx:
                await ctx.info("Gathering detailed storage statistics...")
            try:
                storage_stats = file_manager.get_storage_statistics()
                conversation_count = storage_stats.total_conversations
                solution_count = storage_stats.total_solutions
            except Exception as e:
                logger.warning(f"Failed to get detailed storage statistics: {e}")
        
        # Get backup information
        backup_info = {
            "backups_available": 0,
            "latest_backup": None,
            "total_backup_size_mb": 0.0,
            "backup_directory_status": "unknown"
        }
        
        try:
            backups_dir = storage_paths.get_backups_dir()
            if backups_dir.exists():
                backup_files = list(backups_dir.glob("*backup*"))
                backup_info["backups_available"] = len([f for f in backup_files if f.is_dir()])
                backup_info["backup_directory_status"] = "available"
                
                # Find latest backup
                if backup_files:
                    latest_backup = max(backup_files, key=lambda x: x.stat().st_mtime if x.exists() else 0)
                    backup_info["latest_backup"] = {
                        "path": str(latest_backup),
                        "created_at": datetime.fromtimestamp(latest_backup.stat().st_mtime).isoformat(),
                        "size_mb": round(sum(f.stat().st_size for f in latest_backup.rglob("*") if f.is_file()) / (1024*1024), 2)
                    }
                
                # Calculate total backup size
                try:
                    total_backup_size = sum(
                        sum(f.stat().st_size for f in backup_dir.rglob("*") if f.is_file())
                        for backup_dir in backup_files if backup_dir.is_dir()
                    )
                    backup_info["total_backup_size_mb"] = round(total_backup_size / (1024*1024), 2)
                except Exception:
                    backup_info["total_backup_size_mb"] = 0.0
            else:
                backup_info["backup_directory_status"] = "not_created"
        except Exception as e:
            logger.warning(f"Failed to get backup information: {e}")
            backup_info["backup_directory_status"] = "error"
        
        # System health check
        health_status = {
            "overall_status": "healthy",
            "issues": [],
            "warnings": []
        }
        
        # Check for potential issues
        if not storage_status["permissions_ok"]:
            health_status["issues"].append("Insufficient permissions for some storage directories")
            health_status["overall_status"] = "degraded"
        
        if total_size_mb > 1000:  # > 1GB
            health_status["warnings"].append(f"Storage usage is high: {total_size_mb:.1f} MB")
        
        if storage_stats and storage_stats.total_files > 5000:
            health_status["warnings"].append(f"Large number of files: {storage_stats.total_files}")
        
        if backup_info["backups_available"] == 0:
            health_status["warnings"].append("No backups available")
        
        # Maintenance information
        maintenance_info = {
            "last_maintenance": None,
            "maintenance_needed": False,
            "recommended_actions": []
        }
        
        # Check if maintenance is needed
        if backup_info["backups_available"] > 10:
            maintenance_info["maintenance_needed"] = True
            maintenance_info["recommended_actions"].append("Clean up old backup files")
        
        if storage_stats and storage_stats.disk_usage_mb > 500:
            maintenance_info["maintenance_needed"] = True
            maintenance_info["recommended_actions"].append("Consider archiving old conversations")
        
        # Try to get last maintenance timestamp from metadata
        try:
            metadata_file = storage_paths.get_indexes_dir() / "metadata.json"
            if metadata_file.exists():
                with open(metadata_file, 'r', encoding='utf-8') as f:
                    metadata = json.load(f)
                    maintenance_info["last_maintenance"] = metadata.get("last_maintenance")
        except Exception:
            pass
        
        # Build comprehensive response with enhanced information
        storage_info = {
            # Primary storage locations
            "data_directory": storage_status["storage_paths"]["data"],
            "config_directory": storage_status["storage_paths"]["config"],
            "cache_directory": storage_status["storage_paths"]["cache"],
            
            # Data subdirectories
            "conversations_directory": storage_status["storage_paths"]["conversations"],
            "solutions_directory": storage_status["storage_paths"]["solutions"],
            "indexes_directory": storage_status["storage_paths"]["indexes"],
            "logs_directory": storage_status["storage_paths"]["logs"],
            "backups_directory": storage_status["storage_paths"]["backups"],
            
            # System status
            "initialized": storage_status["initialized"],
            "permissions_ok": storage_status["permissions_ok"],
            "total_storage_size_bytes": storage_status["total_size_bytes"],
            "total_storage_size_mb": round(total_size_mb, 2),
            
            # Directory details
            "directory_status": storage_status["directory_status"],
            
            # Real statistics
            "total_conversations": conversation_count,
            "total_solutions": solution_count,
            "total_files": storage_stats.total_files if storage_stats else 0,
            "average_file_size_kb": round(storage_stats.avg_file_size_kb, 2) if storage_stats else 0,
            
            # Backup information
            "backup_info": backup_info,
            
            # System health
            "health_status": health_status,
            
            # Maintenance information
            "maintenance_info": maintenance_info,
            
            # Application info
            "server_status": "running",
            "version": "1.0.0",
            "platform": sys.platform,
            "last_updated": datetime.now().isoformat()
        }
        
        if ctx:
            await ctx.info(f"Storage information retrieved successfully - Initialized: {storage_status['initialized']}")
        
        return storage_info
        
    except Exception as e:
        if ctx:
            await ctx.error(f"Failed to retrieve storage information: {str(e)}")
        logger.error(f"Failed to retrieve storage information: {str(e)}", exc_info=True)
        raise ValueError(f"Failed to retrieve storage information: {str(e)}")

@mcp.tool()
async def backup_data(
    backup_name: str = None,
    include_cache: bool = False,
    compression: bool = False,
    ctx: Context = None
) -> dict:
    """
    Create a manual backup of all Synapse data.
    
    This tool creates a comprehensive backup of conversations, solutions, indexes,
    and configuration files for disaster recovery purposes.
    
    Args:
        backup_name: Custom name for the backup (optional, auto-generated if not provided)
        include_cache: Whether to include cache files in backup (default: False)
        compression: Whether to compress the backup (default: False - not implemented)
        ctx: MCP context object for logging and progress reporting
        
    Returns:
        dict: Backup result with status, location, and statistics
    """
    try:
        if ctx:
            await ctx.info("Starting manual data backup...")
        
        # Get file manager instance
        file_manager = None
        if ctx and hasattr(ctx, 'request_context') and hasattr(ctx.request_context, 'lifespan_context'):
            file_manager = ctx.request_context.lifespan_context.file_manager
        
        if not file_manager:
            raise RuntimeError("Êó†Ê≥ïËé∑ÂèñFileManagerÂÆû‰æãÔºåËØ∑Ê£ÄÊü•ÊúçÂä°Âô®ÈÖçÁΩÆ")
        
        # Generate backup name if not provided
        if not backup_name:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            backup_name = f"manual_backup_{timestamp}"
        else:
            # Sanitize backup name
            import re
            backup_name = re.sub(r'[^\w\-_.]', '_', backup_name.strip())
            if not backup_name:
                backup_name = f"manual_backup_{int(datetime.now().timestamp())}"
        
        if ctx:
            await ctx.info(f"Creating backup: {backup_name}")
            await ctx.report_progress(progress=0.1, message="Preparing backup...")
        
        # Get pre-backup statistics
        storage_stats = file_manager.get_storage_statistics()
        
        # Create backup directory
        backup_path = file_manager.storage_paths.get_backups_dir() / backup_name
        
        if backup_path.exists():
            raise ValueError(f"Backup already exists: {backup_name}")
        
        if ctx:
            await ctx.info(f"Backup location: {backup_path}")
            await ctx.report_progress(progress=0.3, message="Creating backup directory...")
        
        # Perform the backup
        backup_success = file_manager.export_data(backup_path, include_backups=False)
        
        if not backup_success:
            raise RuntimeError("Backup operation failed - check file manager logs for details")
        
        if ctx:
            await ctx.report_progress(progress=0.8, message="Calculating backup statistics...")
        
        # Calculate backup statistics
        backup_stats = {
            "backup_size_bytes": 0,
            "backup_size_mb": 0.0,
            "files_backed_up": 0,
            "directories_backed_up": 0
        }
        
        try:
            if backup_path.exists():
                for item in backup_path.rglob("*"):
                    if item.is_file():
                        backup_stats["files_backed_up"] += 1
                        backup_stats["backup_size_bytes"] += item.stat().st_size
                    elif item.is_dir():
                        backup_stats["directories_backed_up"] += 1
                
                backup_stats["backup_size_mb"] = round(backup_stats["backup_size_bytes"] / (1024*1024), 2)
        except Exception as e:
            logger.warning(f"Failed to calculate backup statistics: {e}")
        
        # Create backup metadata
        backup_metadata = {
            "backup_name": backup_name,
            "created_at": datetime.now().isoformat(),
            "backup_type": "manual",
            "source_statistics": {
                "conversations_count": storage_stats.total_conversations,
                "solutions_count": storage_stats.total_solutions,
                "total_files": storage_stats.total_files,
                "source_size_mb": round(storage_stats.disk_usage_mb, 2)
            },
            "backup_statistics": backup_stats,
            "include_cache": include_cache,
            "compression": compression,
            "version": "1.0.0"
        }
        
        # Save backup metadata
        try:
            metadata_file = backup_path / "backup_metadata.json"
            with open(metadata_file, 'w', encoding='utf-8') as f:
                json.dump(backup_metadata, f, indent=2, ensure_ascii=False)
        except Exception as e:
            logger.warning(f"Failed to save backup metadata: {e}")
        
        if ctx:
            await ctx.report_progress(progress=1.0, message="Backup completed successfully")
            await ctx.info(f"Backup created successfully: {backup_stats['files_backed_up']} files ({backup_stats['backup_size_mb']} MB)")
        
        result = {
            "success": True,
            "backup_name": backup_name,
            "backup_path": str(backup_path),
            "created_at": datetime.now().isoformat(),
            "backup_type": "manual",
            "statistics": backup_stats,
            "source_data": {
                "conversations_backed_up": storage_stats.total_conversations,
                "solutions_backed_up": storage_stats.total_solutions,
                "total_source_files": storage_stats.total_files
            },
            "options": {
                "include_cache": include_cache,
                "compression": compression
            },
            "summary": f"Successfully created backup '{backup_name}' with {backup_stats['files_backed_up']} files ({backup_stats['backup_size_mb']} MB)"
        }
        
        return result
        
    except Exception as e:
        error_msg = str(e)
        if ctx:
            await ctx.error(f"Backup creation failed: {error_msg}")
        
        logger.error(f"Â§á‰ªΩÂàõÂª∫Â§±Ë¥• - ÂêçÁß∞: {backup_name}, ÈîôËØØ: {error_msg}", exc_info=True)
        
        return {
            "success": False,
            "backup_name": backup_name or "unknown",
            "error": error_msg,
            "error_type": type(e).__name__,
            "created_at": datetime.now().isoformat(),
            "summary": f"Backup creation failed: {error_msg}"
        }

@mcp.tool()
async def restore_backup(
    backup_name: str,
    restore_mode: str = "append",
    verify_backup: bool = True,
    create_restore_backup: bool = True,
    ctx: Context = None
) -> dict:
    """
    Restore Synapse data from a previously created backup.
    
    This tool restores conversations, solutions, and indexes from a backup
    directory with options for verification and pre-restore backup.
    
    Args:
        backup_name: Name of the backup to restore (required)
        restore_mode: Restore strategy - "append" or "overwrite" (default: "append")
        verify_backup: Whether to verify backup integrity before restore (default: True)
        create_restore_backup: Whether to backup current data before restore (default: True)
        ctx: MCP context object for logging and progress reporting
        
    Returns:
        dict: Restore result with status, statistics, and verification results
    """
    try:
        if ctx:
            await ctx.info(f"Starting backup restore: {backup_name}")
        
        # Parameter validation
        if not backup_name or not backup_name.strip():
            raise ValueError("Backup name cannot be empty")
        
        if restore_mode not in ["append", "overwrite"]:
            raise ValueError("Restore mode must be 'append' or 'overwrite'")
        
        # Get file manager instance
        file_manager = None
        if ctx and hasattr(ctx, 'request_context') and hasattr(ctx.request_context, 'lifespan_context'):
            file_manager = ctx.request_context.lifespan_context.file_manager
        
        if not file_manager:
            raise RuntimeError("Êó†Ê≥ïËé∑ÂèñFileManagerÂÆû‰æãÔºåËØ∑Ê£ÄÊü•ÊúçÂä°Âô®ÈÖçÁΩÆ")
        
        # Locate backup directory
        backup_path = file_manager.storage_paths.get_backups_dir() / backup_name.strip()
        
        if not backup_path.exists():
            raise FileNotFoundError(f"Backup not found: {backup_name}")
        
        if not backup_path.is_dir():
            raise ValueError(f"Backup path is not a directory: {backup_name}")
        
        if ctx:
            await ctx.info(f"Found backup at: {backup_path}")
            await ctx.report_progress(progress=0.1, message="Verifying backup...")
        
        # Backup verification
        verification_results = {
            "backup_valid": True,
            "verification_errors": [],
            "backup_metadata": None,
            "conversations_found": 0,
            "solutions_found": 0
        }
        
        if verify_backup:
            try:
                # Check backup structure
                conversations_dir = backup_path / "conversations"
                solutions_dir = backup_path / "solutions"
                indexes_dir = backup_path / "indexes"
                metadata_file = backup_path / "backup_metadata.json"
                
                if not any([conversations_dir.exists(), solutions_dir.exists(), indexes_dir.exists()]):
                    verification_results["backup_valid"] = False
                    verification_results["verification_errors"].append("No valid data directories found in backup")
                
                # Load backup metadata if available
                if metadata_file.exists():
                    try:
                        with open(metadata_file, 'r', encoding='utf-8') as f:
                            verification_results["backup_metadata"] = json.load(f)
                    except Exception as e:
                        verification_results["verification_errors"].append(f"Cannot read backup metadata: {str(e)}")
                
                # Count backup contents
                if conversations_dir.exists():
                    verification_results["conversations_found"] = len(list(conversations_dir.rglob("*.json")))
                
                if solutions_dir.exists():
                    verification_results["solutions_found"] = len(list(solutions_dir.glob("*.json")))
                
                if verification_results["conversations_found"] == 0 and verification_results["solutions_found"] == 0:
                    verification_results["backup_valid"] = False
                    verification_results["verification_errors"].append("No conversation or solution files found in backup")
                
            except Exception as e:
                verification_results["backup_valid"] = False
                verification_results["verification_errors"].append(f"Backup verification failed: {str(e)}")
        
        if verify_backup and not verification_results["backup_valid"]:
            raise ValueError(f"Backup verification failed: {', '.join(verification_results['verification_errors'])}")
        
        if ctx:
            if verification_results["backup_valid"]:
                await ctx.info(f"Backup verified - {verification_results['conversations_found']} conversations, {verification_results['solutions_found']} solutions")
            await ctx.report_progress(progress=0.3, message="Creating pre-restore backup...")
        
        # Create pre-restore backup if requested
        restore_backup_info = None
        if create_restore_backup:
            try:
                restore_backup_timestamp = int(datetime.now().timestamp())
                restore_backup_path = file_manager.storage_paths.get_backups_dir() / f"pre_restore_backup_{restore_backup_timestamp}"
                restore_backup_success = file_manager.export_data(restore_backup_path, include_backups=False)
                
                if restore_backup_success:
                    restore_backup_info = str(restore_backup_path)
                    if ctx:
                        await ctx.info(f"Pre-restore backup created: {restore_backup_path}")
                else:
                    logger.warning("Pre-restore backup failed, continuing with restore")
                    
            except Exception as e:
                logger.warning(f"Pre-restore backup failed: {e}")
        
        if ctx:
            await ctx.info(f"Starting restore with mode: {restore_mode}")
            await ctx.report_progress(progress=0.5, message="Restoring data...")
        
        # Perform the restore using FileManager import
        restore_success = file_manager.import_data(backup_path, merge_mode=restore_mode)
        
        if not restore_success:
            raise RuntimeError("Restore operation failed - check file manager logs for details")
        
        if ctx:
            await ctx.report_progress(progress=0.9, message="Finalizing restore...")
        
        # Get post-restore statistics
        post_restore_stats = file_manager.get_storage_statistics()
        
        if ctx:
            await ctx.report_progress(progress=1.0, message="Restore completed successfully")
            await ctx.info(f"Restore completed - Total: {post_restore_stats.total_conversations} conversations, {post_restore_stats.total_solutions} solutions")
        
        # Build success response
        result = {
            "success": True,
            "backup_name": backup_name,
            "backup_path": str(backup_path),
            "restored_at": datetime.now().isoformat(),
            "restore_mode": restore_mode,
            "verification_performed": verify_backup,
            "pre_restore_backup_created": create_restore_backup,
            "pre_restore_backup_location": restore_backup_info,
            "verification_results": verification_results,
            "statistics": {
                "conversations_after_restore": post_restore_stats.total_conversations,
                "solutions_after_restore": post_restore_stats.total_solutions,
                "total_files_after_restore": post_restore_stats.total_files,
                "total_size_mb_after_restore": round(post_restore_stats.disk_usage_mb, 2)
            },
            "restored_components": {
                "conversations": verification_results["conversations_found"] > 0,
                "solutions": verification_results["solutions_found"] > 0,
                "indexes": True  # Assume indexes are always restored if present
            },
            "summary": f"Successfully restored backup '{backup_name}' using {restore_mode} mode"
        }
        
        return result
        
    except Exception as e:
        error_msg = str(e)
        if ctx:
            await ctx.error(f"Backup restore failed: {error_msg}")
        
        logger.error(f"Â§á‰ªΩÊÅ¢Â§çÂ§±Ë¥• - Â§á‰ªΩ: {backup_name}, ÈîôËØØ: {error_msg}", exc_info=True)
        
        return {
            "success": False,
            "backup_name": backup_name,
            "error": error_msg,
            "error_type": type(e).__name__,
            "restored_at": datetime.now().isoformat(),
            "restore_mode": restore_mode,
            "verification_performed": verify_backup,
            "pre_restore_backup_created": create_restore_backup,
            "verification_results": verification_results if 'verification_results' in locals() else {},
            "summary": f"Restore failed: {error_msg}"
        }

@mcp.tool()
async def extract_solutions(
    conversation_id: str = None,
    extract_type: str = "all",
    min_reusability_score: float = 0.3,
    save_solutions: bool = True,
    overwrite_existing: bool = False,
    ctx: Context = None
) -> dict:
    """
    ‰ªéÂ∑≤‰øùÂ≠òÁöÑÂØπËØùËÆ∞ÂΩï‰∏≠ÊèêÂèñËß£ÂÜ≥ÊñπÊ°àÂπ∂ÂçïÁã¨‰øùÂ≠ò
    
    Ëøô‰∏™Â∑•ÂÖ∑‰ªéÁé∞ÊúâÁöÑÂØπËØùËÆ∞ÂΩï‰∏≠ÊèêÂèñÊâÄÊúâËß£ÂÜ≥ÊñπÊ°àÔºåËøõË°åË¥®ÈáèËØÑ‰º∞ÂíåÂéªÈáçÔºå
    ÁÑ∂ÂêéÂ∞ÜÈ´òË¥®ÈáèÁöÑËß£ÂÜ≥ÊñπÊ°à‰øùÂ≠òÂà∞Áã¨Á´ãÁöÑsolutionsÁõÆÂΩï‰∏≠Ôºå‰æø‰∫éÂêéÁª≠Êü•ÊâæÂíåÈáçÁî®„ÄÇ
    
    ‰ΩøÁî®Âú∫ÊôØÔºö
    1. ÊâπÈáèÊèêÂèñÊâÄÊúâÂØπËØù‰∏≠ÁöÑËß£ÂÜ≥ÊñπÊ°à
    2. ‰ªéÁâπÂÆöÂØπËØù‰∏≠ÊèêÂèñËß£ÂÜ≥ÊñπÊ°à
    3. ÊåâÁ±ªÂûãÁ≠õÈÄâËß£ÂÜ≥ÊñπÊ°àÔºà‰ª£Á†Å„ÄÅÊñπÊ≥ï„ÄÅÊ®°ÂºèÔºâ
    4. Ë¥®ÈáèÁ≠õÈÄâÂíåÂéªÈáçÂ§ÑÁêÜ
    
    Args:
        conversation_id: ÊåáÂÆöÂØπËØùIDÔºå‰∏∫NoneÊó∂Â§ÑÁêÜÊâÄÊúâÂØπËØùËÆ∞ÂΩïÔºàÂèØÈÄâÔºâ
        extract_type: ÊèêÂèñÁ±ªÂûãÁ≠õÈÄâ ("code", "approach", "pattern", "all")
        min_reusability_score: ÊúÄÂ∞èÂèØÈáçÁî®ÊÄßÂàÜÊï∞ÈòàÂÄº (0.0-1.0)
        save_solutions: ÊòØÂê¶Â∞ÜËß£ÂÜ≥ÊñπÊ°à‰øùÂ≠òÂà∞Êñá‰ª∂Á≥ªÁªü (default: True)
        overwrite_existing: ÊòØÂê¶Ë¶ÜÁõñÂ∑≤Â≠òÂú®ÁöÑËß£ÂÜ≥ÊñπÊ°àÊñá‰ª∂ (default: False)
        ctx: MCP‰∏ä‰∏ãÊñáÂØπË±°
        
    Returns:
        dict: ÊèêÂèñÁªìÊûúÔºåÂåÖÂê´Ëß£ÂÜ≥ÊñπÊ°àÂàóË°®„ÄÅÁªüËÆ°‰ø°ÊÅØÂíå‰øùÂ≠òÁä∂ÊÄÅ
        {
            "success": bool,
            "solutions": [...],
            "total_extracted": int,
            "conversations_processed": int,
            "extraction_summary": str,
            "statistics": {...},
            "storage_info": {...}
        }
    """
    try:
        start_time = time.time()
        
        if ctx:
            await ctx.info(f"ÂºÄÂßãËß£ÂÜ≥ÊñπÊ°àÊèêÂèñ‰ªªÂä°")
            if conversation_id:
                await ctx.info(f"ÁõÆÊ†áÂØπËØù: {conversation_id}")
            else:
                await ctx.info("Â§ÑÁêÜÊâÄÊúâÂØπËØùËÆ∞ÂΩï")
        
        # ÂèÇÊï∞È™åËØÅ
        if extract_type not in ["code", "approach", "pattern", "all"]:
            raise ValueError("extract_typeÂøÖÈ°ªÊòØ 'code', 'approach', 'pattern' Êàñ 'all'")
        
        if not isinstance(min_reusability_score, (int, float)) or min_reusability_score < 0 or min_reusability_score > 1:
            raise ValueError("min_reusability_scoreÂøÖÈ°ªÂú®0.0-1.0‰πãÈó¥")
        
        # Ëé∑ÂèñÊèêÂèñËß£ÂÜ≥ÊñπÊ°àÂ∑•ÂÖ∑ÂÆû‰æã
        extract_tool = None
        if ctx and hasattr(ctx, 'request_context') and hasattr(ctx.request_context, 'lifespan_context'):
            extract_tool = ctx.request_context.lifespan_context.extract_solutions_tool
        
        if not extract_tool:
            raise RuntimeError("Êó†Ê≥ïËé∑ÂèñExtractSolutionsToolÂÆû‰æãÔºåËØ∑Ê£ÄÊü•ÊúçÂä°Âô®ÈÖçÁΩÆ")
        
        if ctx:
            await ctx.info(f"‰ΩøÁî®ÂèÇÊï∞: Á±ªÂûã={extract_type}, ÊúÄ‰ΩéË¥®Èáè={min_reusability_score}")
        
        # ÊâßË°åËß£ÂÜ≥ÊñπÊ°àÊèêÂèñ
        result = await extract_tool.extract_solutions(
            conversation_id=conversation_id,
            extract_type=extract_type,
            min_reusability_score=min_reusability_score,
            save_solutions=save_solutions,
            overwrite_existing=overwrite_existing,
            ctx=ctx
        )
        
        # Ê£ÄÊü•ÊèêÂèñÁªìÊûú
        if not result.get("success", False):
            error_msg = result.get("error", "Êú™Áü•ÈîôËØØ")
            raise RuntimeError(f"Ëß£ÂÜ≥ÊñπÊ°àÊèêÂèñÂ§±Ë¥•: {error_msg}")
        
        # ËÆ°ÁÆóÂ§ÑÁêÜÊó∂Èó¥
        processing_time = (time.time() - start_time) * 1000
        result["statistics"]["processing_time_ms"] = round(processing_time, 2)
        
        total_solutions = result.get("total_extracted", 0)
        conversations_processed = result.get("conversations_processed", 0)
        
        if ctx:
            await ctx.info(f"ÊèêÂèñÂÆåÊàê: {total_solutions} ‰∏™Ëß£ÂÜ≥ÊñπÊ°àÊù•Ëá™ {conversations_processed} ‰∏™ÂØπËØù")
            
            if save_solutions:
                files_created = len(result.get("storage_info", {}).get("files_created", []))
                if files_created > 0:
                    await ctx.info(f"Â∑≤‰øùÂ≠òÂà∞ {files_created} ‰∏™Êñá‰ª∂")
            
            # ÊÄßËÉΩ‰ø°ÊÅØ
            if processing_time > 1000:  # > 1Áßí
                await ctx.info(f"Â§ÑÁêÜÊó∂Èó¥: {processing_time:.2f}ms")
            else:
                await ctx.info(f"Â§ÑÁêÜÈ´òÊïà: {processing_time:.2f}ms")
        
        # Â¢ûÂº∫ËøîÂõûÁªìÊûúÊ†ºÂºè
        enhanced_result = {
            # Âü∫Êú¨ÂÖºÂÆπ‰ø°ÊÅØ
            "success": True,
            "solutions": result.get("solutions", []),
            "total_extracted": total_solutions,
            "conversations_processed": conversations_processed,
            "conversations_with_solutions": result.get("conversations_with_solutions", 0),
            "extraction_summary": result.get("extraction_summary", ""),
            
            # Â§ÑÁêÜÂèÇÊï∞‰ø°ÊÅØ
            "extraction_parameters": {
                "conversation_id": conversation_id,
                "extract_type": extract_type,
                "min_reusability_score": min_reusability_score,
                "save_solutions": save_solutions,
                "overwrite_existing": overwrite_existing
            },
            
            # ËØ¶ÁªÜÁªüËÆ°‰ø°ÊÅØ
            "statistics": result.get("statistics", {}),
            
            # Â≠òÂÇ®‰ø°ÊÅØ
            "storage_info": result.get("storage_info", {}),
            
            # ÂÖÉÊï∞ÊçÆ
            "processing_time_ms": round(processing_time, 2),
            "extracted_at": datetime.now().isoformat(),
            "extraction_engine": "synapse_solution_extractor_v1.0"
        }
        
        return enhanced_result
        
    except Exception as e:
        error_msg = str(e)
        if ctx:
            await ctx.error(f"Ëß£ÂÜ≥ÊñπÊ°àÊèêÂèñÂ§±Ë¥•: {error_msg}")
        
        logger.error(f"Ëß£ÂÜ≥ÊñπÊ°àÊèêÂèñÂ§±Ë¥• - ÂØπËØù: {conversation_id}, ÈîôËØØ: {error_msg}", exc_info=True)
        
        # ËøîÂõûÈîôËØØ‰ø°ÊÅØËÄå‰∏çÊòØÊäõÂá∫ÂºÇÂ∏∏
        return {
            "success": False,
            "error": error_msg,
            "error_type": type(e).__name__,
            "solutions": [],
            "total_extracted": 0,
            "conversations_processed": 0,
            "extraction_summary": f"ÊèêÂèñÂ§±Ë¥•: {error_msg}",
            "extraction_parameters": {
                "conversation_id": conversation_id,
                "extract_type": extract_type,
                "min_reusability_score": min_reusability_score,
                "save_solutions": save_solutions,
                "overwrite_existing": overwrite_existing
            },
            "processing_time_ms": 0,
            "extracted_at": datetime.now().isoformat(),
            "extraction_engine": "synapse_solution_extractor_v1.0"
        }

# ==================== ‰∏ªÂÖ•Âè£ÂáΩÊï∞ ====================

async def main():
    """
    Synapse MCP ÊúçÂä°Âô®ÁöÑ‰∏ªÂÖ•Âè£ÁÇπ
    
    Ëøô‰∏™ÂáΩÊï∞ÂêØÂä®MCPÊúçÂä°Âô®Âπ∂Â§ÑÁêÜÂÆ¢Êà∑Á´ØËøûÊé•„ÄÇ
    ÂÆÉËÆæÁΩÆÊó•Âøó„ÄÅÂàùÂßãÂåñÂ≠òÂÇ®Á≥ªÁªüÔºåÁÑ∂ÂêéÂêØÂä®ÊúçÂä°Âô®„ÄÇ
    """
    # ËÆæÁΩÆÊó•Âøó
    setup_logging()
    logger.info("ÂêØÂä® Synapse MCP ÊúçÂä°Âô®...")
    
    try:
        # ËøêË°åFastMCPÊúçÂä°Âô®
        # ÈªòËÆ§‰ΩøÁî®stdio‰º†ËæìÊñπÂºèÔºåËøôÊòØMCPÁöÑÊ†áÂáÜËøûÊé•ÊñπÂºè
        mcp.run()
        
    except KeyboardInterrupt:
        logger.info("Êé•Êî∂Âà∞‰∏≠Êñ≠‰ø°Âè∑ÔºåÊ≠£Âú®ÂÖ≥Èó≠ÊúçÂä°Âô®...")
    except Exception as e:
        logger.error(f"ÊúçÂä°Âô®ËøêË°åÊó∂ÈîôËØØ: {str(e)}", exc_info=True)
        sys.exit(1)

def sync_main():
    """
    Synchronous version of main entry point for compatibility.
    
    This function provides a synchronous interface to start the async server.
    """
    try:
        asyncio.run(main())
    except KeyboardInterrupt:
        print("\nServer stopped")
    except Exception as e:
        print(f"Server startup failed: {str(e)}")
        sys.exit(1)

if __name__ == "__main__":
    sync_main()